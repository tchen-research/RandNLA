{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Active Regression\n",
    "description: Adaptive sampling strategies for linear regression where observation costs are significant\n",
    "keywords: [active regression, adaptive sampling, observation costs, sequential sampling, active learning, optimal design, sample complexity]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 7.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 7.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 7.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 7.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 7.%s\n",
    "    continue: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The active regression problem is a variant of the standard [linear regression problem].\n",
    "\n",
    ":::{prf:definition} Active Linear Regression\n",
    "Given $\\vec{A}\\in\\R^{n\\times d}$ (full rank) and $\\vec{b}\\in\\R^n$, solve\n",
    "\\begin{equation*}\n",
    "\\min_{\\vec{x}} \\| \\vec{b} - \\vec{A}\\vec{x} \\|^2\n",
    ",\\qquad \\vec{A}\\in\\R^{n\\times d}, \\quad \\vec{b}\\in\\R^n,\n",
    "\\end{equation*}\n",
    "using as few entry evaluations of $\\vec{b}$ as possible.\n",
    ":::\n",
    "Since we are measuring cost by the number of entries of $\\vec{b}$ that get observed, most of the algorithm presented in the [chapter](../04-Regression/regression) on linear regression, which require reading the entire vector $\\vec{b}$,  are off the table.\n",
    "\n",
    "This section outlines basic sampling-based approaches to the active regression problem that aim to use a small number of entry evaluations.\n",
    "<!-- \n",
    "\n",
    "### Basis independence\n",
    "\n",
    "For convenience, we will work in an orthonormal basis for $\\range(\\vec{A})$. \n",
    "In particular, let $\\vec{U}$ be a matrix with orthonormal columns such that $\\vec{U}\\vec{C} = \\vec{A}$, for some (invertible) matrix $\\vec{C}$ and consider the regression problem\n",
    "\\begin{equation*}\n",
    "\\min_{\\vec{z}} \\| \\vec{b} - \\vec{U}\\vec{z} \\|^2\n",
    "\\end{equation*}\n",
    "The solution is $\\vec{U}^\\T\\vec{b}$.\n",
    "\n",
    "\n",
    "This change of variables does not impact the residual error nor does it impact sampling $\\vec{b}$.\n",
    "As noted in {prf:ref}`thm-residual-to-error`, residual bounds can be converted to error bounds. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leverage score sampling\n",
    "\n",
    "Recall $\\Call{leverage-dist}(\\vec{A})$ is the distribution that corresponds to sampling an index from $\\{1, \\ldots, n\\}$ proportional to the [Leverage-scores](def:leverage-score) $(\\ell_1, \\ldots, \\ell_n)$ of $\\vec{A}$.\n",
    "\n",
    "\n",
    ":::{prf:algorithm} Regression by Leverage-score sampling\n",
    ":label: alg-leverage-regression\n",
    "**Input:** $\\vec{A}$, $\\vec{b}$, number of samples $k$ \n",
    "\n",
    "1. Sample iid indices $s_1, \\ldots, s_k\\sim \\Call{leverage-dist}(\\vec{A})$\n",
    "1. Form \n",
    "\\begin{equation*}\n",
    "\\widehat{\\vec{A}} := \n",
    "\\begin{bmatrix}\n",
    "- & \\ell_{s_1}^{-1/2}  \\vec{a}_{s_1}^\\T & -\\\\\n",
    "&\\vdots \\\\\n",
    "- & \\ell_{s_k}^{-1/2}  \\vec{a}_{s_k}^\\T & -\\\\\n",
    "\\end{bmatrix}\n",
    ",\\quad\n",
    "\\widehat{\\vec{b}} := \n",
    "\\begin{bmatrix}\n",
    "\\ell_{s_1}^{-1/2}  b_{s_1} \\\\ \\vdots \\\\ \\ell_{s_k}^{-1/2} b_{s_n}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*} \n",
    "1. Obtain solution $\\widehat{\\vec{x}}$ to least squares problem $\\min_{\\vec{x}} \\|\\widehat{\\vec{b}} - \\widehat{\\vec{A}}\\vec{x}\\|$.\n",
    "\n",
    "\n",
    "**Output:** $\\widehat{\\vec{x}}$\n",
    "::: \n",
    "\n",
    "\n",
    "# Analysis\n",
    "\n",
    "Note that {prf:ref}`alg-leverage-regression` is nothing more than {prf:ref}`alg-sketch-and-solve` (sketch-and-solve) using the [Leverage-score Sketch](def:leverage-score-sketch).\n",
    "{prf:ref}`prf-leverage-SE` guarantees that the leverage-score sketch is a subspace embedding for $\\vec{A}$.\n",
    "However, we cannot immediately apply the analysis techniques from used in the analysis of [](../04-Regression/sketch-and-solve), because these require that the sketch is a subspace embedding for $[\\vec{A},\\vec{b}]$. \n",
    "The standard approach to the analysis is to make use of [](./approximate-matrix-multiplication.md) guarantee.\n",
    "\n",
    "\n",
    ":::{prf:theorem}\n",
    "Then for some \n",
    "\\begin{equation*}\n",
    "k = O\\left( d \\log\\left( \\frac{d}{\\delta}\\right) + \\frac{d}{\\delta\\varepsilon} \\right),\n",
    "\\end{equation*}\n",
    "except with probability at most $\\delta$, it holds that, \n",
    "\\begin{equation*}\n",
    "\\| \\vec{b} - \\vec{A}\\widehat{\\vec{x}} \\|^2 \\leq (1+\\varepsilon) \\|\\vec{b} - \\vec{A}\\vec{x}^* \\|^2.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "\n",
    ":::{prf:proof}\n",
    ":class: dropdown\n",
    ":enumerated: false\n",
    "\n",
    "See Raphel's [wiki](https://randnla.github.io/leverage-score-regression/).\n",
    ":::\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
