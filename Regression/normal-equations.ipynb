{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Preconditioned) Normal Equations\n",
    "\n",
    "As noted in the introduction to this chapter, the classical direct approach to solving the linear regression problem {eq}`eqn-regression` is to use a factorization-based solver, which requires $O(nd^2)$ operations.\n",
    "Similar to in our chapter on [Matrix Factorizations](../QR-Factorization/intro.md), we would like to offload the high flop operations to matrix-matrix multiplication, which has a high flop-efficiency.\n",
    "\n",
    "Consider the following observation:\n",
    ":::{prf:theorem}\n",
    ":label: thm:half_pc-normal_eq\n",
    "For any $\\vec{P}$ with $\\range(\\vec{P}) = \\range(\\vec{A})$, one verifies that the solution to {eq}`eqn-regression` is also the solution of the linear system\n",
    "\\begin{equation*}\n",
    "\\vec{P}^\\T\\vec{A}\\vec{x} = \\vec{P}^\\T\\vec{b}.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    ":::{admonition} Proof\n",
    ":class: dropdown\n",
    "It is well-known that the solution to the normal equations $\\vec{A}^\\T\\vec{A}\\vec{x} = \\vec{A}^\\T\\vec{b}$ is also a the solution to the linear regression problem {eq}`eqn-regression`, and any $\\vec{P}$ with $\\range(\\vec{P}) = \\range(\\vec{A})$ can be written as $\\vec{P} = \\vec{A}\\vec{C}$ for some invertible matrix $\\vec{C}$.\n",
    ":::\n",
    "\n",
    "Readers should already be familiar with two special cases:\n",
    "\n",
    "### Normal Equations\n",
    "\n",
    "If $\\vec{P} = \\vec{A}$, then we obtain the standard normal equations. \n",
    "In this case, the dominant cost is the matrix-matrix multiplication $\\vec{A}^\\T\\vec{A}$, which is $O(nd^2)$, but very flop-efficient.\n",
    "However, since $\\cond(\\vec{A}^\\T\\vec{A}) = \\cond(\\vec{A})^2$, this approach can lead to poor accuaracy, when $\\cond(\\vec{A})$ is on the order of the inverse of the *square root* of the machine precision.\n",
    "\n",
    "### QR factorization\n",
    "\n",
    "If $\\vec{P} = \\vec{Q}$, where $\\vec{Q}$ is is the Q factorization in the QR factorization of $\\vec{A}$, then $\\vec{P}^\\T\\vec{A} = \\vec{R}$. \n",
    "In this case we avoid needing to even form $\\vec{P}^\\T\\vec{A}$ explicitly (as long as we also have the $\\vec{R}$ factor for the QR factorization).\n",
    "Moreover, $\\cond(\\vec{R}) = \\cond(\\vec{A})$.\n",
    "\n",
    "We can use the [randomized Choleksy QR algorithm](../QR-Factorization/randomized-cholesky-qr.ipynb) to compute the QR factorization of $\\vec{A}$ in a stable, flop-efficient manner.\n",
    "\n",
    "\n",
    "## Half preconditioned Normal Equations\n",
    "\n",
    "As noted in {cite:p}`ipsen_25`, randomization gives us a third option.\n",
    "Instead of taking $\\vec{P}$ as the Q factor of a QR factorization, we can take $\\vec{P}$ as the approximate orthogonal basis produced by the [Sketched QR algorithm](../QR-Factorization/randomized-cholesky-qr.ipynb#sketched-qr).\n",
    "\n",
    "We suggest implementing this approach as follows:\n",
    "\n",
    ":::{prf:algorithm} Randomized Half Preconditioned Normal Equations\n",
    ":label: rand-HPNE\n",
    "\n",
    "**Input:** $\\vec{A}\\in\\R^{n\\times d}$, $\\vec{b}\\in\\R^n$, sketching dimension $k$\n",
    "\n",
    "1. Get $\\vec{P},\\vec{R}_1 = \\Call{Sketched-QR}(\\vec{A},k)$\n",
    "1. Form $\\vec{X} = \\vec{P}^\\T\\vec{P}$\n",
    "1. Compute Choleksy factorization $\\vec{R} = \\Call{chol}(\\vec{X})$\n",
    "1. $\\vec{R} = \\vec{R}_2\\vec{R}_1$\n",
    "1. Solve $\\vec{R}_2^\\T\\vec{R}\\vec{x} = \\vec{Q}^\\T\\vec{b}$\n",
    "\n",
    "**Output:** $\\vec{x}$\n",
    ":::\n",
    "\n",
    "This approach has a computational profile very similar to Randomized Cholesky-QR.\n",
    "However, note that we avoid the need to compute a triangular solve with a $n\\times d$ matrix.\n",
    "\n",
    "\n",
    "We can easily implement the algorithm in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_HPNE(A,b,k,zeta,rng):\n",
    "\n",
    "    P,R1 = sketched_qr(A,k,zeta,rng)\n",
    "    X = P.T@P\n",
    "    R2 = np.linalg.cholesky(X)\n",
    "    R = R2@R1\n",
    "    y = sp.linalg.solve_triangular(R,P.T@b,lower=False)\n",
    "    x = sp.linalg.solve_triangular(R2.T,y,lower=True)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $\\vec{P}$ is well-conditioned, this approach results in solves with matrices that have a similar condition number as $\\vec{A}$.\n",
    "\n",
    ":::{prf:theorem}\n",
    "\n",
    "Let $\\vec{P}$ be the approximate orthogonal basis produced by the [Sketched QR algorithm](../QR-Factorization/randomized-cholesky-qr.ipynb#sketched-qr).\n",
    "Suppose $\\vec{S}$ is an $\\varepsilon$-subspace embedding for $\\vec{A}$.\n",
    "Then \n",
    "\\begin{equation*}\n",
    "\\cond(\\vec{P}^\\T\\vec{A}) \\leq  \\frac{1+\\varepsilon}{1-\\varepsilon} \\cond(\\vec{A}).\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    ":::{admonition} Proof\n",
    ":class: dropdown\n",
    "\n",
    "By {prf:ref}`sketched-qr-well-conditioned`, we have that \n",
    "\\begin{equation*}\n",
    "\\smin(\\vec{P})  \\geq \\frac{1}{1+\\varepsilon} \n",
    ",\\qquad\n",
    "\\smax(\\vec{P}) \\leq \\frac{1}{1-\\varepsilon}.\n",
    "\\end{equation*}\n",
    "\n",
    "The result then follows from the fact that $\\cond(\\vec{P}^\\T\\vec{A}) \\leq \\cond(\\vec{P})\\cond(\\vec{A})$.\n",
    ":::\n",
    "\n",
    "Thus, the resulting system has a similar condition number as $\\vec{A}$, even if $\\vec{S}$ is only an $\\varepsilon$-subspace embedding for $\\vec{A}$ for some constant $\\varepsilon$ (e.g. $\\varepsilon = 1/2$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Example\n",
    "\n",
    "Let's compare the performance of the three approaches described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def sparse_stack_sketch(n,k,zeta,rng):\n",
    "\n",
    "    k_rem = k%zeta\n",
    "    k_loc = k//zeta+(k_rem>0)\n",
    "\n",
    "    C = np.random.randint(0,k_loc,size=(n,zeta))\n",
    "    if k_rem > 0:\n",
    "        C[:,-1] = np.random.randint(0,k_rem,size=n)\n",
    "    C += np.arange(0,k,k_loc)\n",
    "\n",
    "    indices = C.flatten()\n",
    "    values = np.sqrt(1/zeta)*(2*np.random.randint(2,size=n*zeta)-1)\n",
    "    indptr = np.arange(0,n+1)*zeta\n",
    "    S = sp.sparse.csc_matrix ((values,indices,indptr),shape=(k,n))\n",
    "\n",
    "    return S\n",
    "\n",
    "def cholesky_QR(A):\n",
    "\n",
    "    X = A.T@A\n",
    "    R = np.linalg.cholesky(X).T\n",
    "    Q = sp.linalg.solve_triangular(R.T, A.T, lower=True).T\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "def sketched_qr(A,k,zeta,rng):\n",
    "\n",
    "    n, d = A.shape\n",
    "    S = sparse_stack_sketch(n,k,zeta,rng) \n",
    "    Y = S @ A \n",
    "    R = np.linalg.qr(Y, mode='r') \n",
    "    Q = sp.linalg.solve_triangular(R.T,A.T,lower=True).T \n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "def randomized_cholesky_QR(A,k,zeta,rng):\n",
    "\n",
    "    Q1, R1 = sketched_qr(A,k,zeta,rng)\n",
    "    Q, R2 = cholesky_QR(Q1)\n",
    "    R = R2 @ R1\n",
    "    \n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
