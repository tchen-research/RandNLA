{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Sketch and Solve\n",
    "description: Simple randomized approach to linear regression using sketching to reduce problem size\n",
    "keywords: [sketch and solve, linear regression, Gaussian sketch, subspace embedding, residual bounds, active regression]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "---\n",
    "\n",
    "The sketch-and-solve paradigm is perhaps the simplest randomized approach to the linear regression problem {eq}`task-regression`.\n",
    "This method produces a low-accuracy solution, so it is only suitable for special computational problems where alternatives are not viable. \n",
    "Nevertheless, it is pedagogically important. \n",
    "\n",
    ":::{prf:algorithm} Sketch and Solve\n",
    "**Input:** $\\vec{A}$, $\\vec{b}$, sketching dimension $k$\n",
    "\n",
    "1. Sample $\\vec{S}\\sim\\Call{Sketch}(k,n)$\n",
    "1. Obtain a solution $\\widehat{\\vec{x}}$ to the least-squares problem $\\min_{\\vec{x}} \\|\\vec{S}\\vec{b} - \\vec{S}\\vec{A}\\vec{x}\\|$\n",
    "\n",
    "**Output:** $\\widehat{\\vec{x}}$\n",
    ":::\n",
    "\n",
    "The main question is how to choose the sketching matrix $\\vec{S}$ so that the solution $\\widehat{\\vec{x}}$ is nearly optimal.\n",
    "In particular, we would like that the solution $\\widehat{\\vec{x}}$ returned by the sketch-and-solve algorithm is nearly optimal in the sense that\n",
    "```{math}\n",
    ":label: eqn-sketch-and-solve-guarantee\n",
    "\\|\\vec{b}-\\vec{A}\\widehat{\\vec{x}}\\| \\leq (1+\\varepsilon)\\|\\vec{b}-\\vec{A}\\vec{x}^*\\|,\n",
    "```\n",
    "where $\\vec{x}^*$ is the solution to {eq}`task-regression`.\n",
    "Such a guarantee on the residual is equivalent to a bound on the error.\n",
    ":::{prf:theorem}\n",
    "It holds that\n",
    "\\begin{equation*}\n",
    "\\|\\vec{A}(\\vec{x}^* - \\widehat{\\vec{x}})\\| = \\left(\\|\\vec{b} - \\vec{A}\\widehat{\\vec{x}}\\|^2 - \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|^2\\right)^{1/2}.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "\n",
    ":::{admonition} Proof\n",
    ":class: dropdown\n",
    "\n",
    "Note that the true residual $\\vec{b} - \\vec{A}\\vec{x}^*$ is orthogonal to the range of $\\vec{A}$; i.e., $\\vec{A}^\\T(\\vec{b} - \\vec{A}\\vec{x}^*) = \\vec{0}$. \n",
    "Thus, for any $\\tilde{\\vec{x}} \\in \\R^n$, by the Pythagorean theorem,\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\|\\vec{b} - \\vec{A}\\tilde{\\vec{x}}\\|^2 \n",
    "&= \\|\\vec{b} - \\vec{A}(\\vec{x}^* + \\tilde{\\vec{x}} - \\vec{x}^*)\\|^2\n",
    "\\\\&= \\| \\vec{b} - \\vec{A}\\vec{x}^* - \\vec{A}(\\vec{x}^* - \\tilde{\\vec{x}}) \\|^2\n",
    "\\\\&= \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|^2 + \\|\\vec{A}(\\vec{x}^* - \\tilde{\\vec{x}})\\|^2.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "Rearranging, we find that\n",
    "\\begin{equation*}\n",
    "\\|\\vec{A}(\\vec{x}^* - \\tilde{\\vec{x}})\\| = \\left(\\|\\vec{b} - \\vec{A}\\tilde{\\vec{x}}\\|^2 - \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|^2\\right)^{1/2}.\n",
    "\\end{equation*}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian case\n",
    "\n",
    "To get a handle on the sketch-and-solve algorithm, we will consider the case where the sketching matrix $\\vec{S}$ is a Gaussian random matrix.\n",
    "Note that using a Gaussian sketch results in an algorithm that is not computationally efficient, so other [mixing-based sketching methods](../Sketching/mixing-sketches.md) should be used in practice. \n",
    "\n",
    ":::{prf:theorem}\n",
    "Let $\\vec{S}$ be a Gaussian random matrix with $k$ rows and $n$ columns, and let $\\vec{A} \\in \\R^{m\\times n}$ and $\\vec{b} \\in \\R^m$.\n",
    "Then $\\EE[\\widehat{\\vec{x}}] = \\vec{x}^*$ and\n",
    "\\begin{equation*}\n",
    "\\EE\\left[ \\|\\vec{b} - \\vec{A}\\widehat{\\vec{x}}\\|^2 \\right] \\leq \\left(1+\\frac{d}{k-d-1}\\right) \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|^2.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "Thus we see we solve {eq}`eqn-sketch-and-solve-guarantee` when $k = O(d/\\varepsilon)$.\n",
    "\n",
    ":::{admonition} Proof\n",
    ":class: dropdown\n",
    "\n",
    "Decompose\n",
    "\\begin{equation*}\n",
    "\\vec{b} = \\vec{A}\\vec{A}^+ \\vec{b} + (\\vec{b} - \\vec{A}\\vec{A}^+ \\vec{b} )\n",
    "= \\vec{A}\\vec{x}^* + \\vec{r},\n",
    "\\end{equation*}\n",
    "where $\\vec{r} = \\vec{b} - \\vec{A}\\vec{x}^*$ is the residual vector.\n",
    "We can then write \n",
    "\\begin{equation*}\n",
    "\\widehat{\\vec{x}} = \\vec{x}^* + (\\vec{S}\\vec{A})^+ \\vec{S}(\\vec{b} - \\vec{A}\\vec{A}^+ \\vec{b} ).\n",
    "\\end{equation*}\n",
    "Let $\\vec{U}_1$ be an orthonormal basis for the range of $\\vec{A}$ and and $\\vec{U}_2$ be an orthonormal basis for the orthogonal complement of the range of $\\vec{A}$.\n",
    "Decompose\n",
    "\\begin{equation*}\n",
    "\\vec{S} = \\vec{S}_1\\vec{U}_1^\\T + \\vec{S}_2\\vec{U}_2^\\T,\n",
    "\\qquad \n",
    "\\vec{S}_1 = \\vec{S}\\vec{U}_1,\n",
    "\\quad\n",
    "\\vec{S}_2 = \\vec{S}\\vec{U}_2,\n",
    "\\end{equation*}\n",
    "Since $[\\vec{U}_1 \\, \\vec{U}_2]$ is an orthogonal matrix, by the [Gaussian orthogonal invariance property](../Sketching/Gaussian-sketch.ipynb#prop:gaussian-orthogonal-invariance), we have that $\\vec{S}_1\\sim \\Call{Gaussian}(k,d)$ and $\\vec{S}_2\\sim \\Call{Gaussian}(k,n-d)$ are independent.\n",
    "Moreover, by the optimality of $\\vec{x}^*$, $\\vec{r}$ is orthogonal to the range of $\\vec{A}$.\n",
    "Hence, \n",
    "\\begin{equation*}\n",
    "\\widehat{\\vec{x}} = \\vec{x}^* + (\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}.\n",
    "\\end{equation*}\n",
    "Therefore,\n",
    "\\begin{equation*}\n",
    "\\EE[\\widehat{\\vec{x}}] = \\vec{x}^* + \\EE[(\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+] \\EE[\\vec{S}_2]\\vec{U}_2^\\T\\vec{r} = \\vec{x}^*.\n",
    "\\end{equation*}\n",
    "\n",
    "Next, note that \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\|\\vec{b} - \\vec{A}\\widehat{\\vec{x}}\\|^2\n",
    "&=\\ \\| \\vec{r}  + \\vec{A}(\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2\n",
    "\\\\&= \\|\\vec{r}\\|^2 + \\|\\vec{A}(\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2,\n",
    "\\\\&= \\|\\vec{r}\\|^2 + \\|\\vec{U}_1^\\T\\vec{A}(\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2,\n",
    "\\\\&= \\|\\vec{r}\\|^2 + \\|\\vec{S}_1^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2,\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "where we used the fact that $\\vec{r}$ is orthogonal to the range of $\\vec{A}$, the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem#Inner_product_1paces) and orthogonal invariance of the Euclidean norm, and an identity for the psuedoinverse.\n",
    "\n",
    "Finally, by a [direct computation](../Sketching/Gaussian-sketch.ipynb#prop:gaussian-inverse) and the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem#Inner_product_1paces) and orthogonal invariance of the Euclidean norm\n",
    "\\begin{equation*}\n",
    "\\EE[\\|\\vec{S}_1^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2]\n",
    "= \\EE[ \\| \\vec{S}_1 \\|_\\F^2 ] \\|\\vec{U}_2^\\T\\vec{r}\\|^2\n",
    "= \\frac{d}{k-d-1} \\|\\vec{r}\\|^2.\n",
    "\\end{equation*}\n",
    "Combining the above equations gives the result.\n",
    ":::\n",
    "\n",
    "A similar proof can be found on Ethan's [blog](https://www.ethanepperly.com/index.php/2024/11/19/note-to-self-sketch-and-solve-with-a-gaussian-embedding/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bounds based on Subspace Embeddings\n",
    "\n",
    "If $\\vec{S}$ is a subspace embedding, then it is easy to see that the solution $\\widehat{\\vec{x}}$ returned by the sketch-and-solve algorithm is accurate:\n",
    "\n",
    ":::{prf:theorem} \n",
    ":label: thm-sketch-and-solve\n",
    "Suppose $\\vec{S}$ is an $\\eta$-subspace embedding for $[\\vec{A},\\vec{b}]$.\n",
    "The solution $\\widehat{\\vec{x}}$ returned by the sketch-and-solve algorithm is nearly optimal in the sense that\n",
    "\\begin{equation*}\n",
    "\\|\\vec{b}-\\vec{A}\\widehat{\\vec{x}}\\| \\leq \\frac{1+\\eta}{1-\\eta}\\|\\vec{b}-\\vec{A}\\vec{x}^*\\|.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    ":::{admonition} Proof\n",
    ":class: dropdown\n",
    "\n",
    "Observe that, for any $\\vec{z}\\in\\R^{d}$, $\\vec{b} - \\vec{A}\\vec{z} \\in \\range([\\vec{A}, \\vec{b}])$ and hence, since $\\vec{S}$ is a  subspace embedding for $[\\vec{A},\\vec{b}]$, we have\n",
    "\\begin{equation*}\n",
    "(1-\\eta)\\|\\vec{b} - \\vec{A}\\vec{z}\\|\\leq \\| \\vec{S}(\\vec{b} - \\vec{A}\\vec{z}) \\| \\leq (1+\\eta)\\|\\vec{b} - \\vec{A}\\vec{z}\\|.\n",
    "\\end{equation*}\n",
    "Therefore,\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\|\\vec{b} - \\vec{A}\\widehat{\\vec{x}}\\| \n",
    "&\\leq \\frac{1}{1-\\eta} \\|\\vec{S}(\\vec{b} - \\vec{A}\\widehat{\\vec{x}})\\| \\\\\n",
    "\\\\&= \\frac{1}{1-\\eta} \\min_{\\vec{x}} \\|\\vec{S}(\\vec{b} - \\vec{A}\\vec{x})\\|\n",
    "\\\\&\\leq \\frac{1+\\eta}{1-\\eta} \\min_{\\vec{x}} \\|\\vec{b} - \\vec{A}\\vec{x}\\|.\n",
    "\\\\&= \\frac{1+\\eta}{1-\\eta} \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more refined bound\n",
    "\n",
    "While the proof of {prf:ref}`thm-sketch-and-solve` is extremely simple, it does not recover the rate predicted by our analysis of the Gaussian case (recall a Gaussian matrix is a $\\varepsilon$ subspace embedding when $k = O(d/\\varepsilon^2)$).\n",
    "In fact, as outlined on Ethan's [blog](https://www.ethanepperly.com/index.php/2025/02/12/note-to-self-how-accurate-is-sketch-and-solve/), a more careful analysis actually yields a quadratically better rate\n",
    "\\begin{equation*}\n",
    "\\|\\vec{b}-\\vec{A}\\widehat{\\vec{x}}\\| \\leq (1+4\\eta^2 + O(\\eta^3)) \\min_{\\vec{x}} \\|\\vec{b}-\\vec{A}\\vec{x}\\|.\n",
    "\\end{equation*}\n",
    "Explicit bounds are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Regression\n",
    "\n",
    "An important setting where sketch-and-solve is particularly useful is the [active regression](../Sampling-Based-Methods/active-regression.ipynb) problem.\n",
    "In this task, the cost of the problem is measured by the number of entries of $\\vec{b}$ that we observe. \n",
    "By using a subsampling sketch, we can solve the active regression problem by observing only a few entries of $\\vec{b}$.\n",
    "We discuss this problem in more detail in the Chapter on [Sampling Based Methods](../Sampling-Based-Methods/intro.md) that discusses how to use sketch-and-solve to solve the active regression problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
