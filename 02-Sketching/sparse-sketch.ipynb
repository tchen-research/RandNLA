{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Sparse Sketching Matrices\n",
    "description: Introduction to sparse sketching matrices including CountSketch and SparseStack methods for efficient subspace embeddings with reduced computational cost.\n",
    "keywords: [sparse sketching, CountSketch, SparseStack, subspace embedding, randomized linear algebra, sparse matrices, embedding dimension, Rademacher distribution]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 2.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 2.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 2.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 2.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 2.%s\n",
    "    continue: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse sketching matrices aim to reduce the generate and apply times by making the sketching matrix sparse.\n",
    "\n",
    "\n",
    "## CountSketch\n",
    "\n",
    "The classical example of a sparse sketching matrix is the CountSketch matrix {cite:p}`clarkson_woodruff_13`.\n",
    "\n",
    ":::{prf:definition}\n",
    "We say a matrix $\\vec{S}\\in\\R^{k\\times n}$ is a *CountSketch matrix* if it has the distribution\n",
    "\\begin{equation*}\n",
    "\\vec{S} = \\begin{bmatrix}\n",
    "| & | && |\\\\\n",
    "\\rho_1\\vec{e}_{s_1} & \\rho_2\\vec{e}_{s_2} & \\cdots & \\rho_n\\vec{e}_{s_n} \\\\\n",
    "| & | && |\n",
    "\\end{bmatrix}\n",
    ",\\qquad\n",
    "\\begin{aligned}\n",
    "&\\rho_i\\sim \\Call{Rademacher}\\text{ iid}\\\\\n",
    "&s_i \\sim \\Call{Unif}(\\{1,\\ldots,k\\})\\text{ iid}.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "A CountSketch matrix can be applied to $\\vec{A}$ in $O(\\nnz(\\vec{A}))$ operations.\n",
    "However, it does not provide a good subspace embedding.\n",
    "\n",
    ":::{prf:theorem}\n",
    "\n",
    "Fix any subspace $V\\subset\\R^n$ of dimension $d$.\n",
    "A CountSketch matrix $\\vec{S}$ is a subspace embedding for $V$ with distortion $\\varepsilon$ with constant probability for some\n",
    "\\begin{equation*}\n",
    "k = O\\left( \\frac{d^2}{\\varepsilon^2} \\right).\n",
    "\\end{equation*}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SparseStack Sketch\n",
    "\n",
    "The poor scaling of embedding dimension in CountSketch can be remedied by increasing the per-column sparsity.\n",
    "There are a number of ways to do this, but one of the most promising is to simply stack a bunch of independent CountSketch matrices on top of one another.\n",
    "\n",
    "\n",
    ":::{prf:definition}\n",
    ":label: def:sparse-stack-sketch\n",
    "We say a matrix $\\vec{S}\\in\\R^{k\\times n}$ is a *SparseStack matrix** if it has the distribution\n",
    "\\begin{equation*}\n",
    "\\vec{S} = \\begin{bmatrix}\n",
    "\\rho_{1,1}\\vec{e}_{s_{1,1}} & \\rho_{1,2}\\vec{e}_{s_{1,2}} & \\cdots & \\rho_{1,n}\\vec{e}_{s_{1,n}} \\\\\n",
    "\\rho_{2,1}\\vec{e}_{s_{2,1}} & \\rho_{2,2}\\vec{e}_{s_{2,2}} & \\cdots & \\rho_{2,n}\\vec{e}_{s_{2,n}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\rho_{\\zeta,1}\\vec{e}_{s_{\\zeta,1}} & \\rho_{\\zeta,2}\\vec{e}_{s_{\\zeta,2}} & \\cdots & \\rho_{\\zeta,n}\\vec{e}_{s_{\\zeta,n}}\n",
    "\\end{bmatrix}\n",
    ",\\qquad\n",
    "\\begin{aligned}\n",
    "&\\rho_{i,j}\\sim \\Call{Rademacher}\\text{ iid}\\\\\n",
    "&s_{i,j} \\sim \\Call{Unif}(\\{1,\\ldots,k/\\zeta\\})\\text{ iid}.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "This is equivalent to generating $\\zeta$ CountSketch matrices of hight $k/\\zeta$, and stacking them on top of one another.\n",
    "A SparseStack matrix can be applied to $\\vec{A}$ in $O(\\zeta\\nnz(\\vec{A}))$ operations.\n",
    "\n",
    "SparseStack matrices are known to give a subspace embedding with the optimal embedding dimension, even when $\\zeta$ is extremely small.\n",
    "The following is due to {cite:p}`chenakkod_derezinski_dong_25`.\n",
    "\n",
    ":::{prf:theorem}\n",
    "\n",
    "Fix any subspace $V\\subset\\R^n$ of dimension $d$.\n",
    "A SparseStack matrix $\\vec{S}$ is a subspace embedding for $V$ with distortion $\\varepsilon \\geq d^{-O(1)}$ with constant probability for some\n",
    "\\begin{equation*}\n",
    "k = \\tilde{O}\\left(\\frac{d}{\\varepsilon^2}\\right)\n",
    ",\\quad\n",
    "\\zeta = \\tilde{O}\\left(\\frac{\\log(d)}{\\varepsilon}\\right).\n",
    "\\end{equation*}\n",
    "In this theorem statement, $\\tilde{O}(\\cdot)$ hides sub-polylogarithmic factors.[^otilde]\n",
    ":::\n",
    "\n",
    "[^otilde]: That is, $k = O(d \\log(d)^{o(1)})$ and $\\zeta = O(\\log(d)^{1+o(1)})$.\n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "It's relatively easy to efficiently generate SparseStack matrices in a high-level langauge like Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def sparse_stack_sketch(n,k,zeta,rng):\n",
    "\n",
    "    k_rem = k%zeta\n",
    "    k_loc = k//zeta\n",
    "\n",
    "    C = np.zeros((n,zeta),dtype=int)\n",
    "    C[:,:k_rem] = np.random.randint(0,k_loc+1,size=(n,k_rem))\n",
    "    C[:,k_rem:] = np.random.randint(0,k_loc,size=(n,zeta-k_rem))\n",
    "    offsets = np.cumsum([0]+[k_loc+1]*k_rem + [k_loc]* (zeta-k_rem-1))\n",
    "    C += offsets\n",
    "\n",
    "    indices = C.flatten()\n",
    "    values = np.sqrt(1/zeta)*(2*np.random.randint(2,size=n*zeta)-1)\n",
    "    indptr = np.arange(0,n+1)*zeta\n",
    "    S = sp.sparse.csc_matrix ((values,indices,indptr),shape=(k,n))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comparison with sparse sign sketches\n",
    "\n",
    "A perhaps more common sparse sketching distribution, typically called *SparseSign* matrix, does not separate the rows of the sketch into blocks.\n",
    "Instead, each column has exactly $\\zeta$ random signs in uniformly random positions.\n",
    "We make the following observations:\n",
    "- The current best theoretical bounds for the embedding dimension and sparsity of SparseStack sketches is better than the corresponding bounds for SparseSign sketches.\n",
    "- It is somewhat more tedious SparseSign sketches than to generate than SparseStack sketches, but it can be done efficiently with a bit of care {cite:p}`chen_niroula_ray_subrahmanya_pistoia_kumar_25`.\n",
    "- The embedding dimension of  SparseStack matrices can easily be adjusted by stacking on more CountSketch matrices. \n",
    "This may be useful in practice, where an algorithm might need to adjust the embedding dimension on the fly.\n",
    "\n",
    "\n",
    "Thus, we recommend SparseStack sketches as the default sparse sketch.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
