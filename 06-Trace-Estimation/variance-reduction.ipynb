{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Variance Reduction\n",
    "description: Advanced techniques for reducing variance in trace estimation including Hutch++ and control variates\n",
    "keywords: [variance reduction, Hutch++, control variates, low-rank approximation, deflation, adaptive methods, XTrace]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "---\n",
    "\n",
    "For simplicity, we consider the case when $\\vec{A}$ is symmetric positive definite. \n",
    "In this case, it is reasonable to ask for a *relative* approximation to the trace, that is, we want an estimate of $\\tr(\\vec{A})$ that lives in the interval $[(1-\\varepsilon)\\tr(\\vec{A}), (1+\\varepsilon)\\tr(\\vec{A})]$ for some $\\varepsilon > 0$.\n",
    "\n",
    "The variance of the [Girard-Hutchinson estimator](./girard-hutchinson.ipynb#def:girard_hutchinson_estimator) $\\widehat{\\tr}_m(\\cdot)$ scales as $O(1/m)$, where $m$ is the number of samples used. \n",
    "For instance, if we use $m$ Gaussian test vectors, then\n",
    "```{math}\n",
    ":label: eqn:girard_hutchinson_variance\n",
    "\\EE\\left[\\widehat{\\tr}_m(\\vec{A})\\right] = \\tr(\\vec{A})\n",
    ",\\qquad\n",
    "\\VV\\left[\\widehat{\\tr}_m(\\vec{A})\\right] = \\frac{2 \\Vert\\vec{A}\\Vert_\\F^2}{m}\n",
    "\\leq \\frac{2\\tr(\\vec{A})^2}{m}.\n",
    "```\n",
    "To improve the accuracy of the estimator, we can use variance reduction techniques.\n",
    "\n",
    "\n",
    "The general idea of modern variance reduction techniques {cite:p}`meyer_musco_musco_woodruff_21,persson_cortinovis_kressner_22,epperly_tropp_webber_24` for implicit estimation is to use a control variate.\n",
    "The basic idea is simple. \n",
    ":::{aside} A note on history\n",
    "Interestingly, while this approach to variance reduction approach was popularized by Hutch++, the idea is as old as implicit trace estimation itself!\n",
    "Here is an excerpt from {cite:p}`girard_87`, which may be the first paper on stochastic trace estimation:\n",
    "![](./girard-variance.png)\n",
    ":::\n",
    "Fix any matrix $\\vec{B}$ and consider the estimator\n",
    "```{math}\n",
    "\\widehat{\\tr}_m(\\vec{A},\\vec{B}) := \\tr(\\vec{B}) + \\widehat{\\tr}_m(\\vec{A} - \\vec{B}).\n",
    "```\n",
    "This estimator can be implemented so long a we know $\\tr(\\vec{B})$ and can evaluate the maps $\\vec{x} \\mapsto \\vec{x}^\\T\\vec{A}\\vec{x}$ and $\\vec{x} \\mapsto \\vec{x}^\\T\\vec{B}\\vec{x}$ for any vector $\\vec{x}$.\n",
    "The number of matrix-vector products needed to compute $\\widehat{\\tr}_m(\\vec{A},\\vec{B})$ is the same as for $\\widehat{\\tr}_m(\\vec{A})$, namely $m$.\n",
    "The advantage of the new estimator is that if $\\vec{B}$ is a good approximation of $\\vec{A}$, then\n",
    "```{math}\n",
    "\\VV[\\widehat{\\tr}_m(\\vec{A},\\vec{B})]\n",
    "= \\VV[\\widehat{\\tr}_m(\\vec{A} - \\vec{B})]\n",
    "```\n",
    "can be much smaller than $\\VV[\\widehat{\\tr}_m(\\vec{A})]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hutch++\n",
    "\n",
    "Hutch++ is perhaps the most well-known variance reduction technique for implicit trace estimation.\n",
    "The idea behind the Hutch++ algorithm is to use a low-rank approximation to $\\vec{A}$, computing using the [Randomized SVD](../05-Low-Rank-Approximation/randomized-svd.ipynb) as the control variate. \n",
    "\n",
    "The intuition is simple:\n",
    "- if $\\vec{A}$ has a flat spectrum, then $\\Vert\\vec{A}\\Vert_\\F \\ll \\tr(\\vec{A})$, and regular Girard-Hutchinson estimator works well.\n",
    "- if $\\vec{A}$ has a decaying spectrum, then it is nearly low-rank, and using a the control variate is helpful.\n",
    "\n",
    "Below, we present a simple variant of Hutch++ that admits a simple analysis.\n",
    "The exact variants studied in the original paper differ slightly in the details, but the overall idea is the same.\n",
    "Readers can also look at Raphael's [Blog Post](https://ram900.com/hutchplusplus/) on Hutch++ for more details. \n",
    "\n",
    ":::{prf:algorithm} Hutch++\n",
    ":label: alg:hutchpp\n",
    "\n",
    "**Input**: $\\vec{A}\\in\\R^{n\\times n}$, samples $m$\n",
    "\n",
    "1. Sample $\\vec{\\Omega}\\sim\\Call{Gaussian}(n,(m+2)/4)$\n",
    "1. Compute $\\vec{Y} = \\vec{A}\\vec{\\Omega}$\n",
    "1. Compute QR factorization $\\vec{Q}\\vec{R} = \\Call{qr}(\\vec{Y})$\n",
    "1. Compute $\\vec{Z} = \\vec{A}^\\T\\vec{Q}$\n",
    "1. Define $\\vec{B} = \\vec{Q}\\vec{Z}^\\T$\n",
    "1. Compute $\\widehat{\\tr}_m^{++}(\\vec{A}) := \\tr(\\vec{B}) + \\widehat{\\tr}_{(m-2)/2}(\\vec{A}-\\vec{B})$\n",
    "\n",
    "**Output**: $\\widehat{\\tr}_m^{++}(\\vec{A})$\n",
    ":::\n",
    "\n",
    "Observe that Hutch++ requires $m$ matrix-vector products with $\\vec{A}$. Indeed, $(m+2)/4$ are used to compute $\\vec{A}\\vec{\\Omega}$, $(m+2)/4$ are used to compute $\\vec{Q}^\\T\\vec{A}$, and $(m-2)/2$ are used to compute $\\vec{A}\\vec{x}_i$.\n",
    "\n",
    "\n",
    "\n",
    "Below we implement the Hutch++ estimators. \n",
    "Note that by the cyclic property of trace, $\\tr(\\vec{B}) = \\tr(\\vec{Q}^\\T \\vec{A}\\vec{Q})$.\n",
    "This is more computationally efficient to compute, since $\\vec{Q}^\\T \\vec{A}\\vec{Q}$ is a small matrix while $\\vec{Q}\\vec{Q}^\\T \\vec{A}$ is a large matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hutchpp(A,m):\n",
    "\n",
    "    m1 = (m+2)//4\n",
    "    m2 = m - 2*m1 \n",
    "    n,_ = A.shape\n",
    "\n",
    "    Ω = np.random.randn(n,m1)\n",
    "    Q,_ = np.linalg.qr(A@Ω,mode='reduced')\n",
    "    Z = A.T@Q\n",
    "    trB = np.sum(np.diag(Z.T@Q)) \n",
    "\n",
    "    X = np.random.randn(n,m2)    \n",
    "    trm = np.mean(np.diag(X.T@(A@X) - (X.T@Q)@(Z.T@X)))\n",
    "\n",
    "    return trB + trm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence of Hutch++ compares favorably to {eq}`eqn:girard_hutchinson_variance`.\n",
    "\n",
    ":::{prf:theorem} \n",
    "\n",
    "For any PSD matrix $\\vec{A}\\in\\R^{n\\times n}$, if Hutch++ is implemented with Gaussian queries\n",
    "\\begin{equation*}\n",
    "\\EE\\left[\\widehat{\\tr}_m^{++}(\\vec{A})\\right] = \\tr(\\vec{A})\n",
    ",\\qquad \n",
    "\\VV\\left[\\widehat{\\tr}_m^{++}(\\vec{A})\\right] \\leq \\frac{16}{(m-2)^2} \\tr(\\vec{A})^2.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    ":::{prf:proof}\n",
    ":class: dropdown\n",
    ":enumerated: false\n",
    "\n",
    "Let $k+p$ be the number of samples used to sketch $\\vec{A}$ in order to build the low-rank approximation, and $\\ell$ be the number of samples used to estimate the trace of $\\vec{A} - \\vec{B}$.\n",
    "\n",
    "As noted above, $\\EE[\\widehat{\\tr}_m^{++}(\\vec{A}) ] = \\EE[\\EE[\\widehat{\\tr}_m^{++}(\\vec{A}) | \\vec{B} ]] = \\tr(\\vec{A})$ and $\\VV[\\widehat{\\tr}_m^{++}(\\vec{A})|\\vec{B}] = \\VV[\\widehat{\\tr}_m(\\vec{A} - \\vec{B})|\\vec{B}] = 2\\Vert\\vec{A} - \\vec{B}\\Vert_\\F^2/\\ell$.\n",
    "Now, by {prf:ref}`thm-rsvd-frob`, a standard bound for the Randomized SVD, we have\n",
    "```{math}\n",
    "\\EE\\left[\\Vert\\vec{A} - \\vec{B}\\Vert_\\F^2\\right]\n",
    "= \\EE\\left[\\Vert\\vec{A} - \\vec{Q}\\vec{Q}^\\T\\vec{A}\\Vert_\\F^2\\right]\n",
    "\\leq \\left( 1 + \\frac{k}{p-1}\\right) \\Vert \\vec{A} - \\llbracket \\vec{A} \\rrbracket_k \\Vert_\\F^2.\n",
    "```\n",
    "As proved in Lemma 7 in {cite:p}`gilbert_strauss_tropp_vershynin_07`,\n",
    "```{math}\n",
    "\\Vert \\vec{A} - \\llbracket \\vec{A} \\rrbracket_k \\Vert_\\F\n",
    "\\leq \\frac{1}{2\\sqrt{k}} \\tr(\\vec{A}).\n",
    "```\n",
    "Thus, setting $p=1$,\n",
    "```{math}\n",
    "\\VV[\\widehat{\\tr}_m^{++}(\\vec{A})|\\vec{B}]\n",
    "\\leq \\frac{2}{\\ell}\\left( 1 + \\frac{k}{p-1}\\right) \\frac{1}{4k} \\tr(\\vec{A})^2\n",
    "= \\frac{1}{\\ell k} \\tr(\\vec{A})^2.\n",
    "```\n",
    "Finally, the choices $k = (m-2)/4$ and $\\ell = (m-2)/2$ yield the stated bound.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Numerical Example\n",
    "\n",
    "Let's set up a numerical experiment to compare Hutch++ to the Girard--Hutchinson estimator.\n",
    "We'll compare the performance on a problem with fast spectral decay and and one with slow spectral decay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import fft,sparse\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from randnla import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Set up the test problem\n",
    "# ========================\n",
    "\n",
    "n = 3000\n",
    "Λ_fast = 1/np.arange(1,n+1)**2\n",
    "Λ_slow = 1/np.arange(1,n+1)**1\n",
    "\n",
    "\n",
    "problems = [\n",
    "    {'name':'fast decay',\n",
    "     'Λ':Λ_fast},\n",
    "    {'name':'slow decay',\n",
    "     'Λ':Λ_slow}\n",
    "]\n",
    "\n",
    "n_ms = 5\n",
    "ms = np.geomspace(10,1000,n_ms,dtype='int')\n",
    "\n",
    "methods = [\n",
    "    {'name':'Girard-Hutchinson',\n",
    "     'func':girard_hutchinson},\n",
    "    {'name':'Hutch++',\n",
    "     'func':hutchpp}\n",
    "]\n",
    "\n",
    "# ========================\n",
    "# Now run the experiment\n",
    "# ========================\n",
    "n_repeat = 20\n",
    "errors = {}\n",
    "for problem in problems:\n",
    "    problem_name = problem['name']\n",
    "    Λ = problem['Λ']\n",
    "    A = sp.sparse.diags(Λ)\n",
    "    tr_true = np.sum(Λ)\n",
    "\n",
    "    for method in methods:\n",
    "\n",
    "        alg_name = method['name']\n",
    "        alg = method['func']\n",
    "\n",
    "        err = np.zeros((n_ms,n_repeat))\n",
    "        for i,m in enumerate(ms):\n",
    "            for j in range(n_repeat):\n",
    "\n",
    "                err[i,j] = np.abs(alg(A,m) - tr_true) / tr_true\n",
    "\n",
    "        errors[problem_name,alg_name] = err\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Plot the results\n",
    "# ========================\n",
    " \n",
    "σ = .1\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(10, 4),sharey=True)\n",
    "\n",
    "for i,problem in enumerate(problems):\n",
    "\n",
    "    problem_name = problem['name']\n",
    "    ax = axs[i]\n",
    "    ax.set_title(problem_name)\n",
    "    for method in methods:\n",
    "\n",
    "        alg_name = method['name']\n",
    "\n",
    "        err = errors[problem_name,alg_name]\n",
    "\n",
    "        bot,mid,top = np.quantile(err, [σ, .5, 1-σ], axis=1)\n",
    "\n",
    "        ax.plot(ms,mid,label=alg_name)\n",
    "        ax.fill_between(ms,bot,top,alpha=.2)\n",
    "\n",
    "\n",
    "        ax.set_xlabel('matvecs $m$')\n",
    "        ax.set_xscale('log')\n",
    "\n",
    "\n",
    "axs[0].set_ylabel(r'error: $|\\operatorname{tr}(A) - \\mathrm{alg}|/\\operatorname{tr}(A)$')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('hutchpp.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are as expected. \n",
    "When the eigenvalue decay is fast, then Hutch++ performs substantially better than the Girard-Hutchinson estimator since it can effectively remove much of the mass with the low-rank approximation.\n",
    "\n",
    "![](./hutchpp.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants and Related Methods\n",
    "\n",
    "While Hutch++ admits a simple analysis, it's clear there is lots of room for modifications/improvements. \n",
    "Below we highlight some of the main variants and related works.\n",
    "\n",
    "### Non-adaptive methods\n",
    "\n",
    "It's possible to do the variance reduction step in a non-adaptive way; i.e. without multiple passes over $\\vec{A}$.\n",
    "Specifically, the [Randomized SVD](../05-Low-Rank-Approximation/randomized-svd.ipynb) can be replaced with [Generalized Nyström](../05-Low-Rank-Approximation/generalized-nystrom.ipynb).\n",
    "\n",
    "\n",
    "### Deflation\n",
    "\n",
    "Several methods have proposed to use compute the top eigencomponents of $\\vec{A}$ explicitly and apply a randomized estimator to the residual {cite:p}`weisse_wellein_alvermann_fehske_06,gambhir_stathopoulos_orginos_17,morita_tohyama_20`.\n",
    "\n",
    "### Adaptive Hutch++\n",
    "\n",
    "In Hutch++, the number of queries used for low-rank approximation and estimating the trace of the residual are roughly balanced. \n",
    "However, there may be better tradeoffs between the two depending on the spectral decay of $\\vec{A}$.\n",
    "In {cite:p}`persson_cortinovis_kressner_22`, the authors propose an adaptive variant of Hutch++ that attempts to use more queries for the low-rank approximation when the spectrum is flat, and more queries for estimating the trace of the residual when the spectrum is decaying.\n",
    "\n",
    "### XTrace\n",
    "\n",
    "The XTrace method {cite:p}`epperly_tropp_webber_24` is based on the *exchangability principle* uses all $m$ queries in the same way.\n",
    "In particular, the method uses all but one query for low-rank approximation, and the last query to estimate the trace of the residual, and then averages this estimate over all possible choices of the last query.\n",
    "By using a clever low-rank update formula, the algorithm can also be made computationally efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
