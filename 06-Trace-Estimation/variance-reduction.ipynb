{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Variance Reduction\n",
    "description: Advanced techniques for reducing variance in trace estimation including Hutch++ and control variates\n",
    "keywords: [variance reduction, Hutch++, control variates, low-rank approximation, deflation, adaptive methods, XTrace]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "---\n",
    "\n",
    "For simplicity, we consider the case when $\\vec{A}$ is symmetric positive definite. \n",
    "In this case, it is reasonable to ask for a *relative* approximation to the trace, that is, we want an estimate of $\\tr(\\vec{A})$ that lives in the interval $[(1-\\varepsilon)\\tr(\\vec{A}), (1+\\varepsilon)\\tr(\\vec{A})]$ for some $\\varepsilon > 0$.\n",
    "\n",
    "The variance of the [Girard-Hutchinson estimator](./girard-hutchinson.ipynb#def:girard_hutchinson_estimator) $\\widehat{\\tr}_m(\\cdot)$ scales as $O(1/m)$, where $m$ is the number of samples used. \n",
    "For instance, if we use $m$ Gaussian test vectors, then\n",
    "```{math}\n",
    ":label: eqn:girard_hutchinson_variance\n",
    "\\EE\\left[\\widehat{\\tr}_m(\\vec{A})\\right] = \\tr(\\vec{A})\n",
    ",\\qquad\n",
    "\\VV\\left[\\widehat{\\tr}_m(\\vec{A})\\right] = \\frac{2 \\Vert\\vec{A}\\Vert_\\F^2}{m}\n",
    "\\leq \\frac{2\\tr(\\vec{A})^2}{m}.\n",
    "```\n",
    "To improve the accuracy of the estimator, we can use variance reduction techniques.\n",
    "\n",
    "The general idea of modern variance reduction techniques {cite:p}`meyer_musco_musco_woodruff_21,persson_cortinovis_kressner_22,epperly_tropp_webber_24` for implicit estimation is to use a control variate.\n",
    "The basic idea is simple. \n",
    "Fix any matrix $\\vec{B}$ and consider the estimator\n",
    "```{math}\n",
    "\\widehat{\\tr}_m(\\vec{A},\\vec{B}) := \\tr(\\vec{B}) + \\widehat{\\tr}_m(\\vec{A} - \\vec{B}).\n",
    "```\n",
    "This estimator can be implemented so long a we know $\\tr(\\vec{B})$ and can evaluate the maps $\\vec{x} \\mapsto \\vec{x}^\\T\\vec{A}\\vec{x}$ and $\\vec{x} \\mapsto \\vec{x}^\\T\\vec{B}\\vec{x}$ for any vector $\\vec{x}$.\n",
    "The number of matrix-vector products needed to compute $\\widehat{\\tr}_m(\\vec{A},\\vec{B})$ is the same as for $\\widehat{\\tr}_m(\\vec{A})$, namely $m$.\n",
    "The advantage of the new estimator is that if $\\vec{B}$ is a good approximation of $\\vec{A}$, then\n",
    "```{math}\n",
    "\\VV[\\widehat{\\tr}_m(\\vec{A},\\vec{B})]\n",
    "= \\VV[\\widehat{\\tr}_m(\\vec{A} - \\vec{B})]\n",
    "```\n",
    "can be much smaller than $\\VV[\\widehat{\\tr}_m(\\vec{A})]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hutch++\n",
    "\n",
    "Hutch++ is perhaps the most well-known variance reduction technique for implicit trace estimation.\n",
    "The idea behind the Hutch++ algorithm is to use a low-rank approximation to $\\vec{A}$, computing using the [Randomized SVD](../05-Low-Rank-Approximation/randomized-svd.ipynb) as the control variate. \n",
    "\n",
    "The intuition is simple:\n",
    "- if $\\vec{A}$ has a flat spectrum, then $\\Vert\\vec{A}\\Vert_\\F \\ll \\tr(\\vec{A})$, and regular Girard-Hutchinson estimator works well.\n",
    "- if $\\vec{A}$ has a decaying spectrum, then it is nearly low-rank, and using a the control variate is helpful.\n",
    "\n",
    "Below, we present a simple variant of Hutch++ that admits a simple analysis.\n",
    "The exact variants studied in the original paper differ slightly in the details, but the overall idea is the same.\n",
    "Readers can also look at Raphael's [Blog Post](https://ram900.com/hutchplusplus/) on Hutch++ for more details. \n",
    "\n",
    "````{prf:algorithm} Hutch++\n",
    ":label: alg:hutchpp\n",
    "\n",
    "**Input**: $\\vec{A}\\in\\R^{n\\times n}$, samples $m$\n",
    "\n",
    "1. Sample $\\vec{\\Omega}\\sim\\Call{Gaussian}(n,(m+2)/4)$\n",
    "1. Compute orthonormal basis $\\vec{Q}$ for $\\vec{A}\\vec{\\Omega}$\n",
    "1. Define $\\vec{B} = \\vec{Q}\\vec{Q}^\\T\\vec{A}$\n",
    "1. Compute $\\widehat{\\tr}_m^{++}(\\vec{A}) := \\tr(\\vec{B}) + \\frac{2}{m-2}\\sum_{i=1}^{(m-2)/2} \\vec{x}_i^\\T (\\vec{A}-\\vec{B})\\vec{x}_i$, where $\\vec{x}_i$ are iid Gaussian vectors.\n",
    "\n",
    "**Output**: $\\widehat{\\tr}_m^{++}(\\vec{A})$\n",
    "````\n",
    "\n",
    "Observe that Hutch++ requires $m$ matrix-vector products with $\\vec{A}$. Indeed, $(m+2)/4$ are used to compute $\\vec{A}\\vec{S}$, $(m+2)/4$ are used to compute $\\vec{Q}^\\T\\vec{A}$, and $(m-2)/2$ are used to compute $\\vec{A}\\vec{x}_i$.\n",
    "\n",
    "The convergence of Hutch++ compares favorably to {eq}`eqn:girard_hutchinson_variance`.\n",
    "\n",
    "````{prf:theorem} \n",
    "\n",
    "For any PSD matrix $\\vec{A}\\in\\R^{n\\times n}$, if Hutch++ is implemented with Gaussian queries\n",
    "```{math}\n",
    "\\EE\\left[\\widehat{\\tr}_m^{++}(\\vec{A})\\right] = \\tr(\\vec{A})\n",
    ",\\qquad \n",
    "\\VV\\left[\\widehat{\\tr}_m^{++}(\\vec{A})\\right] \\leq \\frac{16}{(m-2)^2} \\tr(\\vec{A})^2.\n",
    "```\n",
    "````\n",
    "\n",
    "````{prf:proof}\n",
    ":class: dropdown\n",
    ":enumerated: false\n",
    "\n",
    "Let $k+p$ be the number of samples used to sketch $\\vec{A}$ in order to build the low-rank approximation, and $\\ell$ be the number of samples used to estimate the trace of $\\vec{A} - \\vec{B}$.\n",
    "\n",
    "As noted above, $\\EE[\\widehat{\\tr}_m^{++}(\\vec{A}) ] = \\EE[\\EE[\\widehat{\\tr}_m^{++}(\\vec{A}) | \\vec{B} ]] = \\tr(\\vec{A})$ and $\\VV[\\widehat{\\tr}_m^{++}(\\vec{A})|\\vec{B}] = \\VV[\\widehat{\\tr}_m(\\vec{A} - \\vec{B})|\\vec{B}] = 2\\Vert\\vec{A} - \\vec{B}\\Vert_\\F^2/\\ell$.\n",
    "Now, by standard bounds for the Randomized SVD (TODO, LINK TO RSVD), we have\n",
    "```{math}\n",
    "\\EE\\left[\\Vert\\vec{A} - \\vec{B}\\Vert_\\F^2\\right]\n",
    "= \\EE\\left[\\Vert\\vec{A} - \\vec{Q}\\vec{Q}^\\T\\vec{A}\\Vert_\\F^2\\right]\n",
    "\\leq \\left( 1 + \\frac{k}{p-1}\\right) \\Vert \\vec{A} - \\llbracket \\vec{A} \\rrbracket_k \\Vert_\\F^2.\n",
    "```\n",
    "As proved in Lemma 7 in {cite:p}`gilbert_strauss_tropp_vershynin_07`,\n",
    "```{math}\n",
    "\\Vert \\vec{A} - \\llbracket \\vec{A} \\rrbracket_k \\Vert_\\F\n",
    "\\leq \\frac{1}{2\\sqrt{k}} \\tr(\\vec{A}).\n",
    "```\n",
    "Thus, setting $p=1$,\n",
    "```{math}\n",
    "\\VV[\\widehat{\\tr}_m^{++}(\\vec{A})|\\vec{B}]\n",
    "\\leq \\frac{2}{\\ell}\\left( 1 + \\frac{k}{p-1}\\right) \\frac{1}{4k} \\tr(\\vec{A})^2\n",
    "= \\frac{1}{\\ell k} \\tr(\\vec{A})^2.\n",
    "```\n",
    "Finally, the choices $k = (m-2)/4$ and $\\ell = (m-2)/2$ yield the stated bound.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Numerical Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def hutchpp(A):\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods\n",
    "\n",
    "### Deflation\n",
    "\n",
    "Several methods have proposed to use compute the top eigencomponents of $\\vec{A}$ explicitly and apply a randomized estimator to the residual {cite:p}`weisse_wellein_alvermann_fehske_06,gambhir_stathopoulos_orginos_17,morita_tohyama_20`.\n",
    "\n",
    "### Adaptive Hutch++\n",
    "\n",
    "In Hutch++, the number of queries used for low-rank approximation and estimating the trace of the residual are roughly balanced. \n",
    "However, there may be better tradeoffs between the two depending on the spectral decay of $\\vec{A}$.\n",
    "In {cite:p}`persson_cortinovis_kressner_22`, the authors propose an adaptive variant of Hutch++ that attempts to use more queries for the low-rank approximation when the spectrum is flat, and more queries for estimating the trace of the residual when the spectrum is decaying.\n",
    "\n",
    "### XTrace\n",
    "\n",
    "The XTrace method {cite:p}`epperly_tropp_webber_24` is based on the *exchangability principle* uses all $m$ queries in the same way.\n",
    "In particular, the method uses all but one query for low-rank approximation, and the last query to estimate the trace of the residual, and then averages this estimate over all possible choices of the last query.\n",
    "By using a clever low-rank update formula, the algorithm can also be made computationally efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Interestingly, while popularized this Variance reduction approach was popularized by Hutch++, this idea is as old as trace estimatino itself!\n",
    "Here is an excerpt from {cite:p}`girard_87`, which may be the first paper on stochastic trace estimation:\n",
    "\n",
    "```{figure} ./girard-variance.png\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
