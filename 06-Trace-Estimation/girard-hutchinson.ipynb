{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Girard-Hutchinson Estimator\n",
    "description: Fundamental unbiased estimator for matrix traces using random vectors and variance analysis\n",
    "keywords: [Girard-Hutchinson estimator, Hutchinson trace estimator, unbiased estimator, variance bounds, Gaussian vectors, Rademacher vectors]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "---\n",
    "\n",
    "## Girard-Hutchinson Estimator\n",
    "\n",
    "Let $\\vec{x}$ be a random vector satisfying $\\mathbb{E}[\\vec{x}] = \\vec{0}$ and $\\mathbb{E}[\\vec{x}\\vec{x}^\\T] = \\vec{I}$.\n",
    "Then, a direct computations reveals that $\\vec{x}^\\T\\vec{A}\\vec{x}$ is an unbiased estimator of the trace of $\\vec{A}$. \n",
    "In particular, by the cylic property of the trace and linearity of expectation,\n",
    "\\begin{equation}\n",
    "\\EE[ \\vec{x}^\\T \\vec{A}\\vec{x}] \n",
    "= \\EE[ \\tr(\\vec{x}^\\T \\vec{A}\\vec{x}) ] \n",
    "= \\EE[ \\tr(\\vec{A}\\vec{x}\\vec{x}^\\T ) ] \n",
    "= \\tr(\\vec{A}\\EE[ \\vec{x}\\vec{x}^\\T ] )\n",
    "= \\text{tr}(\\vec{A}).\n",
    "\\end{equation}\n",
    "This suggests a simple randomized estimator.\n",
    ":::{prf:definition} Girard-Hutchinson Estimator\n",
    ":label: def:girard_hutchinson_estimator\n",
    "Fix an integer $m \\geq 1$. The *Girard-Hutchinson trace estimator* is\n",
    "\\begin{equation*}\n",
    "\\widehat{\\tr}_m(\\vec{A}) := \\frac{1}{m}\\sum_{i=1}^{m} \\vec{x}_i^\\T \\vec{A} \\vec{x}_i,\n",
    "\\end{equation*}\n",
    "where $\\vec{x}_i$ are iid copies of some vector $\\vec{x}$ satisfying $\\mathbb{E}[\\vec{x}] = \\vec{0}$ and $\\mathbb{E}[\\vec{x}\\vec{x}^\\T] = \\vec{I}$.\n",
    ":::\n",
    "```{aside} History\n",
    "This estimator is often referred to as *Hutchinson's trace estimator*, especially when $\\vec{x}$ is a random Rademacher vector.\n",
    "However, {cite:p}`hutchinson_89` itself cites {cite:p}`girard_87` which addresses the same task by using samples of $\\vec{x}$ drawn uniformly from the unit hypersphere.\n",
    "See {cite:p}`chen_22` for more discussion.\n",
    "```\n",
    "\n",
    "We can implement the Girard-Hutchinson estimator. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def girard_hutchinson(A,m):\n",
    "\n",
    "    n,_ = A.shape\n",
    "\n",
    "    X = np.random.randn(n,m)\n",
    "    trm = np.mean(np.diag(X.T@(A@X)))\n",
    "\n",
    "    return trm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Variance Bounds\n",
    "\n",
    "We can easily characterize the expectation and variance of the Girard-Hutchinson estimator when $\\vec{x}$ is a Gaussian vector.\n",
    ":::{prf:theorem}\n",
    "Suppose $\\vec{A}$ is symmatric and $\\vec{x}_i$ are iid Gaussian vectors. \n",
    "Then\n",
    "\\begin{equation*}\n",
    "\\EE[ \\widehat{\\tr}_m(\\vec{A}) ] = \\tr(\\vec{A}),\n",
    "\\qquad\n",
    "\\VV[ \\widehat{\\tr}_m(\\vec{A}) ] = \\frac{2\\|\\vec{A}\\|_\\F^2}{m}.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "\n",
    ":::{admonition} Proof\n",
    ":class: dropdown\n",
    "A standard computation reveals that \n",
    "\\begin{equation*}\n",
    "\\EE[ \\widehat{\\tr}_m(\\vec{A}) ] = \\tr(\\vec{A}),\n",
    "\\qquad\n",
    "\\VV[ \\widehat{\\tr}_m(\\vec{A}) ] = \\frac{1}{m} \\VV[\\vec{x}^\\T\\vec{A}\\vec{x}].\n",
    "\\end{equation*}\n",
    "\n",
    "Since $\\vec{A}$ is symmetric, it has an eigendecomposition $\\vec{A} = \\vec{U}\\vec{\\Lambda}\\vec{U}^\\T$, where $\\vec{U}\\in\\R^{n\\times n}$ is an orthogonal matrix and $\\vec{\\Lambda} = \\text{diag}(\\lambda_1,\\ldots,\\lambda_n)$ is a diagonal matrix.\n",
    "By the [Gaussian orthogonal invariance property](../02-Sketching/Gaussian-sketch.ipynb#prop:gaussian-orthogonal-invariance), $\\vec{z} = \\vec{U}\\vec{x}$ is also a Gaussian vector.\n",
    "Observe that \n",
    "\\begin{equation*}\n",
    "\\vec{x}^\\T\\vec{A}\\vec{x} = \\vec{z}^\\T\\vec{\\Lambda}\\vec{z} = \\sum_{i=1}^{n} \\lambda_i z_i^2.\n",
    "\\end{equation*}\n",
    "Since $z_i^2$ are independent $\\chi^2_1$ random variables, we have,\n",
    "\\begin{equation*}\n",
    "\\VV[\\vec{x}^\\T\\vec{A}\\vec{x}] = \\sum_{i=1}^{n} \\lambda_i^2 \\VV[z_i^2] = \\sum_{i=1}^{n} \\lambda_i^2 \\cdot 2 = 2\\|\\vec{A}\\|_\\F^2.\n",
    "\\end{equation*}\n",
    ":::\n",
    "If $\\vec{A}$ is not symmetric, we can use the fact that $\\widehat{\\tr}_m(\\vec{A})=\\widehat{\\tr}_m((\\vec{A} + \\vec{A}^\\T)/2)$.\n",
    "\n",
    "### Other Distributions\n",
    "\n",
    "Besides Gaussian, two commonly used distributions are:\n",
    "- **Rademacher:** Each component of $\\vec{x}$ is iid $\\{-1,1\\}$ with equal probability. \n",
    "- **Random unit vector (real):** Each component of $\\vec{x}$ is iid $\\mathcal{N}(0,1)$, and then $\\vec{x}$ is normalized to have norm $\\sqrt{n}$.\n",
    "\n",
    "When $\\vec{A}$ is symmetric, the variance of the estimator can be computed explicitly:\n",
    "| Distribution of $\\vec{x}$ | Variance |\n",
    "|-------|-----|\n",
    "| Gaussian     | $\\frac{1}{m} \\cdot 2 \\Vert\\vec{A}\\Vert_\\F^2$ |\n",
    "|  Rademacher  | $\\frac{1}{m} \\cdot 2 (\\Vert\\vec{A}\\Vert_\\F^2 - \\sum_i A_{ii}^2)$ |\n",
    "| Uniform (real)     | $\\frac{1}{m} \\cdot \\frac{2n}{n+2} \\left( \\Vert \\vec{A} \\Vert_\\F^2 - \\frac{1}{n}\\tr(\\vec{A})^2 \\right)$ |\n",
    "\n",
    "Deriving the variance for Rademacher vectors is a straightforward (but tedious) exercise in basic probability. \n",
    "The variance for the uniform distribution can also be derived from elementary techniques, but requires some care to handle the normalization. \n",
    "Readers can refer to Ethan's [blog post](https://www.ethanepperly.com/index.php/2023/01/26/stochastic-trace-estimation/) for a derivation of the variances listed above, as well as some additional discussion on interesting optimality properties of certain distributions.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
