{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Girard-Hutchinson Estimator\n",
    "description: Fundamental unbiased estimator for matrix traces using random vectors and variance analysis\n",
    "keywords: [Girard-Hutchinson estimator, Hutchinson trace estimator, unbiased estimator, variance bounds, Gaussian vectors, Rademacher vectors]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 6.%s\n",
    "    continue: true\n",
    "---\n",
    "\n",
    "Let $\\vec{x}$ be a random vector satisfying $\\mathbb{E}[\\vec{x}] = \\vec{0}$ and $\\mathbb{E}[\\vec{x}\\vec{x}^\\T] = \\vec{I}$.\n",
    "Then, a direct computations reveals that $\\vec{x}^\\T\\vec{A}\\vec{x}$ is an unbiased estimator of the trace of $\\vec{A}$. \n",
    "In particular, by the cylic property of the trace and linearity of expectation,\n",
    "\\begin{equation}\n",
    "\\EE[ \\vec{x}^\\T \\vec{A}\\vec{x}] \n",
    "= \\EE[ \\tr(\\vec{x}^\\T \\vec{A}\\vec{x}) ] \n",
    "= \\EE[ \\tr(\\vec{A}\\vec{x}\\vec{x}^\\T ) ] \n",
    "= \\tr(\\vec{A}\\EE[ \\vec{x}\\vec{x}^\\T ] )\n",
    "= \\text{tr}(\\vec{A}).\n",
    "\\end{equation}\n",
    "This suggests a simple randomized estimator.\n",
    ":::{prf:definition} Girard-Hutchinson Estimator\n",
    ":label: def:girard_hutchinson_estimator\n",
    "Fix an integer $m \\geq 1$. The *Girard-Hutchinson trace estimator* is\n",
    "\\begin{equation*}\n",
    "\\widehat{\\tr}_m(\\vec{A}) := \\frac{1}{m}\\sum_{i=1}^{m} \\vec{x}_i^\\T \\vec{A} \\vec{x}_i,\n",
    "\\end{equation*}\n",
    "where $\\vec{x}_i$ are iid copies of some vector $\\vec{x}$ satisfying $\\mathbb{E}[\\vec{x}] = \\vec{0}$ and $\\mathbb{E}[\\vec{x}\\vec{x}^\\T] = \\vec{I}$.\n",
    ":::\n",
    "```{aside} History\n",
    "This estimator is often referred to as *Hutchinson's trace estimator*, especially when $\\vec{x}$ is a random Rademacher vector.\n",
    "However, {cite:p}`hutchinson_89` itself cites {cite:p}`girard_87` which addresses the same task by using samples of $\\vec{x}$ drawn uniformly from the unit hypersphere.\n",
    "See {cite:p}`chen_22` for more discussion.\n",
    "```\n",
    "\n",
    "We can implement the Girard-Hutchinson estimator. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def girard_hutchinson(A,m):\n",
    "\n",
    "    n,_ = A.shape\n",
    "\n",
    "    X = np.random.randn(n,m)\n",
    "    trm = np.mean(np.diag(X.T@(A@X)))\n",
    "\n",
    "    return trm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above implementation we use that if $\\vec{X} = [\\vec{x}_1, \\ldots, \\vec{x}_m] \\in \\mathbb{R}^{n \\times m}$ then \n",
    ":::{math}\n",
    "\\vec{X}^\\T \\vec{A} \\vec{X} =\n",
    "\\begin{bmatrix}\n",
    "\\vec{x}_1^\\T \\vec{A} \\vec{x}_1 & \\vec{x}_1^\\T \\vec{A} \\vec{x}_2 & \\cdots & \\vec{x}_1^\\T \\vec{A} \\vec{x}_m \\\\\n",
    "\\vec{x}_2^\\T \\vec{A} \\vec{x}_1 & \\vec{x}_2^\\T \\vec{A} \\vec{x}_2 & \\cdots & \\vec{x}_2^\\T \\vec{A} \\vec{x}_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\vec{x}_m^\\T \\vec{A} \\vec{x}_1 & \\vec{x}_m^\\T \\vec{A} \\vec{x}_2 & \\cdots & \\vec{x}_m^\\T \\vec{A} \\vec{x}_m\n",
    "\\end{bmatrix},\n",
    ":::\n",
    "and hence $\\widehat{\\tr}_m(\\vec{A}) = \\frac{1}{m} \\tr(\\vec{X}^\\T \\vec{A} \\vec{X})$.\n",
    "\n",
    "This approach unnecessarily computes and stores the off-diagonal entries of $\\vec{X}^\\T \\vec{A} \\vec{X}$.\n",
    "However, in trace estimation we typically view products with $\\vec{A}$ as expensive, so the additional cost of forming $\\vec{X}^\\T \\vec{A} \\vec{X}$ is often negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Variance Bounds\n",
    "\n",
    "We can easily characterize the expectation and variance of the Girard-Hutchinson estimator when $\\vec{x}$ is a Gaussian vector.\n",
    ":::{prf:theorem}\n",
    ":label: thm-girard_hutchinson_variance\n",
    "Suppose $\\vec{A}$ is symmetric and $\\vec{x}_i$ are iid Gaussian vectors. \n",
    "Then\n",
    "\\begin{equation*}\n",
    "\\EE[ \\widehat{\\tr}_m(\\vec{A}) ] = \\tr(\\vec{A}),\n",
    "\\qquad\n",
    "\\VV[ \\widehat{\\tr}_m(\\vec{A}) ] = \\frac{2\\|\\vec{A}\\|_\\F^2}{m}.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "\n",
    ":::{prf:proof}\n",
    ":class: dropdown\n",
    ":enumerated: false\n",
    "\n",
    "A standard computation reveals that \n",
    "\\begin{equation*}\n",
    "\\EE[ \\widehat{\\tr}_m(\\vec{A}) ] = \\tr(\\vec{A}),\n",
    "\\qquad\n",
    "\\VV[ \\widehat{\\tr}_m(\\vec{A}) ] = \\frac{1}{m} \\VV[\\vec{x}^\\T\\vec{A}\\vec{x}].\n",
    "\\end{equation*}\n",
    "\n",
    "Since $\\vec{A}$ is symmetric, it has an eigendecomposition $\\vec{A} = \\vec{U}\\vec{\\Lambda}\\vec{U}^\\T$, where $\\vec{U}\\in\\R^{n\\times n}$ is an orthogonal matrix and $\\vec{\\Lambda} = \\text{diag}(\\lambda_1,\\ldots,\\lambda_n)$ is a diagonal matrix.\n",
    "By the [Gaussian orthogonal invariance property](../02-Sketching/Gaussian-sketch.ipynb#prop:gaussian-orthogonal-invariance), $\\vec{z} = \\vec{U}\\vec{x}$ is also a Gaussian vector.\n",
    "Observe that \n",
    "\\begin{equation*}\n",
    "\\vec{x}^\\T\\vec{A}\\vec{x} = \\vec{z}^\\T\\vec{\\Lambda}\\vec{z} = \\sum_{i=1}^{n} \\lambda_i z_i^2.\n",
    "\\end{equation*}\n",
    "Since $z_i^2$ are independent $\\chi^2_1$ random variables, we have,\n",
    "\\begin{equation*}\n",
    "\\VV[\\vec{x}^\\T\\vec{A}\\vec{x}] = \\sum_{i=1}^{n} \\lambda_i^2 \\VV[z_i^2] = \\sum_{i=1}^{n} \\lambda_i^2 \\cdot 2 = 2\\|\\vec{A}\\|_\\F^2.\n",
    "\\end{equation*}\n",
    ":::\n",
    "If $\\vec{A}$ is not symmetric, we can use the fact that $\\widehat{\\tr}_m(\\vec{A})=\\widehat{\\tr}_m((\\vec{A} + \\vec{A}^\\T)/2)$.\n",
    "\n",
    "One can also derive probability bounds; see e.g. {cite:p}`cortinovis_kressner_21`.\n",
    "\n",
    "### Other Distributions\n",
    "\n",
    "Besides Gaussian, two commonly used distributions are:\n",
    "- **Rademacher:** Each component of $\\vec{x}$ is iid $\\{-1,1\\}$ with equal probability. \n",
    "- **Random unit vector (real):** Each component of $\\vec{x}$ is iid $\\mathcal{N}(0,1)$, and then $\\vec{x}$ is normalized to have norm $\\sqrt{n}$.\n",
    "\n",
    "When $\\vec{A}$ is symmetric, the variance of the estimator can be computed explicitly:\n",
    "| Distribution of $\\vec{x}$ | Variance |\n",
    "|-------|-----|\n",
    "| Gaussian     | $\\frac{1}{m} \\cdot 2 \\Vert\\vec{A}\\Vert_\\F^2$ |\n",
    "|  Rademacher  | $\\frac{1}{m} \\cdot 2 (\\Vert\\vec{A}\\Vert_\\F^2 - \\sum_i A_{ii}^2)$ |\n",
    "| Uniform (real)     | $\\frac{1}{m} \\cdot \\frac{2n}{n+2} \\left( \\Vert \\vec{A} \\Vert_\\F^2 - \\frac{1}{n}\\tr(\\vec{A})^2 \\right)$ |\n",
    "\n",
    "Deriving the variance for Rademacher vectors is a straightforward (but tedious) exercise in basic probability. \n",
    "The variance for the uniform distribution can also be derived from elementary techniques, but requires some care to handle the normalization. \n",
    "Readers can refer to Ethan's [blog post](https://www.ethanepperly.com/index.php/2023/01/26/stochastic-trace-estimation/) for a derivation of the variances listed above, as well as some additional discussion on interesting optimality properties of certain distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Diagonal Approximation\n",
    "\n",
    "A task closely related to trace estimation is approximating the diagonal of a matrix $\\vec{A}\\in\\mathbb{R}^{n\\times n}$.\n",
    "The analog of [](def:girard_hutchinson_estimator) is the Girard-Hutchinson estimator for diagonal estimation:\n",
    "\n",
    ":::{prf:definition} Girard-Hutchinson diagonal estimator\n",
    ":label: def:diagonal-gh\n",
    "\n",
    "Fix an integer $m\\geq 0$.\n",
    "The Girard-Hutchinson diagonal estimator is:\n",
    "\\begin{equation*}\n",
    "\\widehat{\\operatorname{diag}}_m(\\vec{A}) = \\frac{1}{m} \\sum_{i=1}^m \\vec{x}_i \\odot (\\vec{A} \\vec{x}_i)\n",
    "\\end{equation*}\n",
    "where $\\vec{x}_i$ are iid copies of some vector $\\vec{x}$ satisfying $\\mathbb{E}[\\vec{x}] = \\vec{0}$ and $\\mathbb{E}[\\vec{x}\\vec{x}^\\T] = \\vec{I}$ and $\\odot$ denotes the Hadamard (element-wise) product.\n",
    ":::\n",
    "\n",
    "It's not too hard to analyze the mean and variance to prove a result analogous to [](thm-girard_hutchinson_variance).\n",
    "See also {cite:p}`dharangutte_musco_23` for a probability bound.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
