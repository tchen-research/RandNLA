{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Sketch and Solve\n",
    "description: Simple randomized approach to linear regression using sketching to reduce problem size\n",
    "keywords: [sketch and solve, linear regression, Gaussian sketch, subspace embedding, residual bounds, active regression]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "---\n",
    "\n",
    "The sketch-and-solve paradigm is perhaps the simplest randomized approach to the linear regression problem {eq}`task-regression`.\n",
    "This method produces a low-accuracy solution, so it is only suitable for special computational problems where alternatives are not viable. \n",
    "Nevertheless, it is pedagogically important. \n",
    "\n",
    ":::{prf:algorithm} Sketch and Solve\n",
    ":label: alg-sketch-and-solve\n",
    "**Input:** $\\vec{A}$, $\\vec{b}$, sketching dimension $k$\n",
    "\n",
    "1. Sample $\\vec{S}\\sim\\Call{Sketch}(k,n)$\n",
    "1. Obtain a solution $\\widehat{\\vec{x}}$ to the least-squares problem $\\min_{\\vec{x}} \\|\\vec{S}\\vec{b} - \\vec{S}\\vec{A}\\vec{x}\\|$\n",
    "\n",
    "**Output:** $\\widehat{\\vec{x}}$\n",
    ":::\n",
    "\n",
    "We would like to choose the sketching matrix $\\vec{S}$ so that the solution $\\widehat{\\vec{x}}$ is nearly optimal, in the sense that\n",
    "```{math}\n",
    ":label: eqn-sketch-and-solve-guarantee\n",
    "\\|\\vec{b}-\\vec{A}\\widehat{\\vec{x}}\\| \\approx \\|\\vec{b}-\\vec{A}\\vec{x}^*\\|,\n",
    "```\n",
    "where $\\vec{x}^*$ is the solution to {eq}`task-regression`.\n",
    "Such a guarantee on the residual is equivalent to a bound on the error.\n",
    ":::{prf:theorem}\n",
    ":label: thm-residual-to-error\n",
    "Let $\\vec{x}^*$ be the solution to {eq}`task-regression`.\n",
    "It holds that\n",
    "\\begin{equation*}\n",
    "\\|\\vec{A}(\\vec{x}^* - \\widehat{\\vec{x}})\\| = \\left(\\|\\vec{b} - \\vec{A}\\widehat{\\vec{x}}\\|^2 - \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|^2\\right)^{1/2}.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "\n",
    ":::{prf:proof}\n",
    ":class: dropdown\n",
    ":enumerated: false\n",
    "\n",
    "Note that the true residual $\\vec{b} - \\vec{A}\\vec{x}^*$ is orthogonal to the range of $\\vec{A}$; i.e., $\\vec{A}^\\T(\\vec{b} - \\vec{A}\\vec{x}^*) = \\vec{0}$. \n",
    "Thus, for any $\\tilde{\\vec{x}} \\in \\R^n$, by the Pythagorean theorem,\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\|\\vec{b} - \\vec{A}\\tilde{\\vec{x}}\\|^2 \n",
    "&= \\|\\vec{b} - \\vec{A}(\\vec{x}^* + \\tilde{\\vec{x}} - \\vec{x}^*)\\|^2\n",
    "\\\\&= \\| \\vec{b} - \\vec{A}\\vec{x}^* - \\vec{A}(\\vec{x}^* - \\tilde{\\vec{x}}) \\|^2\n",
    "\\\\&= \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|^2 + \\|\\vec{A}(\\vec{x}^* - \\tilde{\\vec{x}})\\|^2.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "Rearranging, we find that\n",
    "\\begin{equation*}\n",
    "\\|\\vec{A}(\\vec{x}^* - \\tilde{\\vec{x}})\\| = \\left(\\|\\vec{b} - \\vec{A}\\tilde{\\vec{x}}\\|^2 - \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|^2\\right)^{1/2}.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "### Active Regression\n",
    "\n",
    "An important setting where sketch-and-solve is particularly useful is the [active regression](../07-Sampling-Based-Methods/active-regression.ipynb) problem.\n",
    "In this task, the cost of the problem is measured by the number of entries of $\\vec{b}$ that we observe. \n",
    "By using a subsampling sketch, we can solve the active regression problem by observing only a few entries of $\\vec{b}$.\n",
    "We discuss this problem in more detail in the Chapter on [Sampling Based Methods](../07-Sampling-Based-Methods/intro.md) that discusses how to use sketch-and-solve to solve the active regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian case\n",
    "\n",
    "To get a handle on the sketch-and-solve algorithm, we will consider the case where the sketching matrix $\\vec{S}$ is a Gaussian random matrix.\n",
    "Note that using a Gaussian sketch results in an algorithm that is not computationally efficient, so other [mixing-based sketching methods](../02-Sketching/mixing-sketches.md) should be used in practice. \n",
    "\n",
    ":::{prf:theorem}\n",
    "Let $\\vec{S}$ be a Gaussian random matrix with $k$ rows and $n$ columns, and let $\\vec{A} \\in \\R^{m\\times n}$ and $\\vec{b} \\in \\R^m$.\n",
    "Then $\\EE[\\widehat{\\vec{x}}] = \\vec{x}^*$ and\n",
    "\\begin{equation*}\n",
    "\\EE\\left[ \\|\\vec{b} - \\vec{A}\\widehat{\\vec{x}}\\|^2 \\right] \\leq \\left(1+\\frac{d}{k-d-1}\\right) \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|^2.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "Thus we see we solve {eq}`eqn-sketch-and-solve-guarantee` when $k = O(d/\\varepsilon)$.\n",
    "\n",
    ":::{prf:proof}\n",
    ":class: dropdown\n",
    ":enumerated: false\n",
    "\n",
    "Decompose\n",
    "\\begin{equation*}\n",
    "\\vec{b} = \\vec{A}\\vec{A}^+ \\vec{b} + (\\vec{b} - \\vec{A}\\vec{A}^+ \\vec{b} )\n",
    "= \\vec{A}\\vec{x}^* + \\vec{r},\n",
    "\\end{equation*}\n",
    "where $\\vec{r} = \\vec{b} - \\vec{A}\\vec{x}^*$ is the residual vector.\n",
    "We can then write \n",
    "\\begin{equation*}\n",
    "\\widehat{\\vec{x}} = \\vec{x}^* + (\\vec{S}\\vec{A})^+ \\vec{S}(\\vec{b} - \\vec{A}\\vec{A}^+ \\vec{b} ).\n",
    "\\end{equation*}\n",
    "Let $\\vec{U}_1$ be an orthonormal basis for the range of $\\vec{A}$ and and $\\vec{U}_2$ be an orthonormal basis for the orthogonal complement of the range of $\\vec{A}$.\n",
    "Decompose\n",
    "\\begin{equation*}\n",
    "\\vec{S} = \\vec{S}_1\\vec{U}_1^\\T + \\vec{S}_2\\vec{U}_2^\\T,\n",
    "\\qquad \n",
    "\\vec{S}_1 = \\vec{S}\\vec{U}_1,\n",
    "\\quad\n",
    "\\vec{S}_2 = \\vec{S}\\vec{U}_2,\n",
    "\\end{equation*}\n",
    "Since $[\\vec{U}_1 \\, \\vec{U}_2]$ is an orthogonal matrix, by the [Gaussian orthogonal invariance property](../02-Sketching/Gaussian-sketch.ipynb#prop:gaussian-orthogonal-invariance), we have that $\\vec{S}_1\\sim \\Call{Gaussian}(k,d)$ and $\\vec{S}_2\\sim \\Call{Gaussian}(k,n-d)$ are independent.\n",
    "Moreover, by the optimality of $\\vec{x}^*$, $\\vec{r}$ is orthogonal to the range of $\\vec{A}$.\n",
    "Hence, \n",
    "\\begin{equation*}\n",
    "\\widehat{\\vec{x}} = \\vec{x}^* + (\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}.\n",
    "\\end{equation*}\n",
    "Therefore,\n",
    "\\begin{equation*}\n",
    "\\EE[\\widehat{\\vec{x}}] = \\vec{x}^* + \\EE[(\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+] \\EE[\\vec{S}_2]\\vec{U}_2^\\T\\vec{r} = \\vec{x}^*.\n",
    "\\end{equation*}\n",
    "\n",
    "Next, note that \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\|\\vec{b} - \\vec{A}\\widehat{\\vec{x}}\\|^2\n",
    "&=\\ \\| \\vec{r}  + \\vec{A}(\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2\n",
    "\\\\&= \\|\\vec{r}\\|^2 + \\|\\vec{A}(\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2,\n",
    "\\\\&= \\|\\vec{r}\\|^2 + \\|\\vec{U}_1^\\T\\vec{A}(\\vec{S}_1\\vec{U}_1^\\T\\vec{A})^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2,\n",
    "\\\\&= \\|\\vec{r}\\|^2 + \\|\\vec{S}_1^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2,\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "where we used the fact that $\\vec{r}$ is orthogonal to the range of $\\vec{A}$, the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem#Inner_product_1paces) and orthogonal invariance of the Euclidean norm, and an identity for the psuedoinverse.\n",
    "\n",
    "Finally, by a [direct computation](../02-Sketching/Gaussian-sketch.ipynb#prop:gaussian-inverse) and the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem#Inner_product_1paces) and orthogonal invariance of the Euclidean norm\n",
    "\\begin{equation*}\n",
    "\\EE[\\|\\vec{S}_1^+ \\vec{S}_2\\vec{U}_2^\\T\\vec{r}\\|^2]\n",
    "= \\EE[ \\| \\vec{S}_1 \\|_\\F^2 ] \\|\\vec{U}_2^\\T\\vec{r}\\|^2\n",
    "= \\frac{d}{k-d-1} \\|\\vec{r}\\|^2.\n",
    "\\end{equation*}\n",
    "Combining the above equations gives the result.\n",
    ":::\n",
    "\n",
    "A similar proof can be found on Ethan's [blog](https://www.ethanepperly.com/index.php/2024/11/19/note-to-self-sketch-and-solve-with-a-gaussian-embedding/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounds based on Subspace Embeddings\n",
    "\n",
    "If $\\vec{S}$ is a subspace embedding, then it is easy to see that the solution $\\widehat{\\vec{x}}$ returned by the sketch-and-solve algorithm is accurate:\n",
    "\n",
    ":::{prf:theorem} \n",
    ":label: thm-sketch-and-solve\n",
    "Suppose $\\vec{S}$ is an $\\varepsilon$-subspace embedding for $[\\vec{A},\\vec{b}]$.\n",
    "The solution $\\widehat{\\vec{x}}$ returned by the sketch-and-solve algorithm is nearly optimal in the sense that\n",
    "\\begin{equation*}\n",
    "\\|\\vec{b}-\\vec{A}\\widehat{\\vec{x}}\\| \\leq \\frac{1+\\varepsilon}{1-\\varepsilon}\\|\\vec{b}-\\vec{A}\\vec{x}^*\\|.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    ":::{prf:proof}\n",
    ":class: dropdown\n",
    ":enumerated: false\n",
    "\n",
    "Observe that, for any $\\vec{z}\\in\\R^{d}$, $\\vec{b} - \\vec{A}\\vec{z} \\in \\range([\\vec{A}, \\vec{b}])$ and hence, since $\\vec{S}$ is a  subspace embedding for $[\\vec{A},\\vec{b}]$, we have\n",
    "\\begin{equation*}\n",
    "(1-\\varepsilon)\\|\\vec{b} - \\vec{A}\\vec{z}\\|\\leq \\| \\vec{S}(\\vec{b} - \\vec{A}\\vec{z}) \\| \\leq (1+\\varepsilon)\\|\\vec{b} - \\vec{A}\\vec{z}\\|.\n",
    "\\end{equation*}\n",
    "Therefore,\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\|\\vec{b} - \\vec{A}\\widehat{\\vec{x}}\\| \n",
    "&\\leq \\frac{1}{1-\\varepsilon} \\|\\vec{S}(\\vec{b} - \\vec{A}\\widehat{\\vec{x}})\\| \\\\\n",
    "\\\\&= \\frac{1}{1-\\varepsilon} \\min_{\\vec{x}} \\|\\vec{S}(\\vec{b} - \\vec{A}\\vec{x})\\|\n",
    "\\\\&\\leq \\frac{1+\\varepsilon}{1-\\varepsilon} \\min_{\\vec{x}} \\|\\vec{b} - \\vec{A}\\vec{x}\\|.\n",
    "\\\\&= \\frac{1+\\varepsilon}{1-\\varepsilon} \\|\\vec{b} - \\vec{A}\\vec{x}^*\\|.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    ":::\n",
    "### A more refined bound\n",
    "\n",
    "While the proof of {prf:ref}`thm-sketch-and-solve` is extremely simple, it does not recover the rate predicted by our analysis of the Gaussian case (recall a Gaussian matrix is a $\\varepsilon$ subspace embedding when $k = O(d/\\varepsilon^2)$).\n",
    "In fact, as outlined on Ethan's [blog](https://www.ethanepperly.com/index.php/2025/02/12/note-to-self-how-accurate-is-sketch-and-solve/), a more careful analysis actually yields a quadratically better rate\n",
    "\\begin{equation*}\n",
    "\\|\\vec{b}-\\vec{A}\\widehat{\\vec{x}}\\| \\leq (1+4\\varepsilon^2 + O(\\varepsilon^3)) \\min_{\\vec{x}} \\|\\vec{b}-\\vec{A}\\vec{x}\\|.\n",
    "\\end{equation*}\n",
    "Explicit bounds are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Numerical Experiment\n",
    "\n",
    "Let's look at the accuracy of the sketch-and-solve algorithm as a function of the sketching dimension $k$ for the different [mixing](../02-Sketching/mixing-sketches.md)-based sketches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import fft,sparse\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from randnla import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Set up the test problem\n",
    "# ========================\n",
    "n = 2**16\n",
    "d = 100\n",
    "\n",
    "U, s, Vt = np.linalg.svd(np.random.rand(n, d), full_matrices=False)\n",
    "s = np.geomspace(1e-4, 1, d)  # Controlled singular values for numerical stability\n",
    "A = U @ np.diag(s) @ Vt\n",
    "\n",
    "b_norm = 1\n",
    "residual_norm = .1\n",
    "\n",
    "v = np.random.randn(n)\n",
    "v_span = U @ (U.T @ v)\n",
    "v_perp = v - v_span\n",
    "v_span /= np.linalg.norm(v_span)\n",
    "v_perp /= np.linalg.norm(v_perp)\n",
    "\n",
    "b = v_span * np.sqrt(b_norm**2 - residual_norm**2) + v_perp * residual_norm\n",
    "Ax_true = v_span * np.sqrt(b_norm**2 - residual_norm**2)\n",
    "\n",
    "# ========================\n",
    "# Now run the experiment\n",
    "# ========================\n",
    "sketch_methods = {\n",
    "    'Gaussian': {\n",
    "        'func': lambda k,rng: gaussian_sketch(n,k,rng),\n",
    "    },\n",
    "    'Trig': {\n",
    "        'func': lambda k,rng: trig_sketch(n,k,rng),\n",
    "    },\n",
    "    'Sparse (zeta=4)': {\n",
    "        'func': lambda k,rng: sparse_stack_sketch(n,k,4,rng),\n",
    "    },\n",
    "    'Sparse (zeta=8)': {\n",
    "        'func': lambda k,rng: sparse_stack_sketch(n,k,8,rng),\n",
    "    },\n",
    "}\n",
    "\n",
    "ks = np.geomspace(2*d,3000,10,dtype=int)\n",
    "\n",
    "n_repeat = 10\n",
    "results = {}\n",
    "\n",
    "for method_name, method_info in sketch_methods.items():\n",
    "\n",
    "    errors = np.zeros((n_repeat,len(ks)))\n",
    "    for i in range(n_repeat):\n",
    "        rng = np.random.RandomState(i)\n",
    "        for j,k in enumerate(ks):\n",
    "        \n",
    "            S = method_info['func'](k,rng)\n",
    "            \n",
    "            x = np.linalg.lstsq(S@A,S@b,rcond=None)[0]\n",
    "            errors[i,j] = np.linalg.norm(Ax_true-A@x)\n",
    "\n",
    "    results[method_name] = {\n",
    "        'error': errors,\n",
    "    }\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Plot the results\n",
    "# ========================\n",
    " \n",
    "σ = 0.1\n",
    "\n",
    "for method_name, method_info in sketch_methods.items():\n",
    "    # Get the error data for this problem and method\n",
    "    error = results[method_name]['error']\n",
    "    \n",
    "    bot, mid, top = np.quantile(error, [σ, .5, 1-σ], axis=0)\n",
    "    plt.plot(ks/d, mid, label=method_name, linewidth=2)\n",
    "    plt.fill_between(ks/d, bot, top, alpha=.2)\n",
    "\n",
    "# Theoretical bound\n",
    "plt.plot(ks/d, np.sqrt(d/(ks-d-1))*residual_norm, ls=':', color='k', label=r'Gaussian expectation')\n",
    "\n",
    "plt.ylabel(r'accuracy: $\\|A(x^*-\\widehat{x})\\|^2$')\n",
    "plt.xlabel('Sketch size ($k/d$)')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('sketch_and_solve.svg')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "![Performance comparison plot showing accuracy vs sketch size for different sketching methods in sketch-and-solve. Log-log plot shows error decreasing as sketch size increases, with all methods (Gaussian, Trigonometric, Sparse variants) performing similarly and following the theoretical Gaussian expectation bound. The y-axis shows solution accuracy and x-axis shows sketch size relative to problem dimension.](./sketch_and_solve.svg)\n\nSimilar to as we observed in our [numerical study](../02-Sketching/which-sketch-should-i-use.ipynb) of the embedding dimension, all sketching distributions behave very similarly to the Gaussian sketch."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}