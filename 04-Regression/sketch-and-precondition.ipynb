{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Sketch and Precondition\n",
    "description: High-accuracy linear regression using randomized sketching for preconditioning with detailed coverage of preconditioned iterative methods\n",
    "keywords: [sketch and precondition, preconditioning, LSQR, condition number, sketched QR, convergence acceleration, randomized preconditioning]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "---\n",
    "\n",
    "The sketch-and-precondition approach solves {eq}`task-regression` by using sketching to build a high-quality preconditioner.\n",
    "Unlike sketch-and-solve, which may sacrifice accuracy for speed, sketch-and-precondition maintains high accuracy while achieving significant computational speedups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Preconditioned Iterative Methods\n",
    "\n",
    "Given any invertible matrix $\\vec{M}\\in\\R^{d\\times d}$ (called a *preconditioner*), solving {eq}`task-regression` is equivalent to solving\n",
    "```{math}\n",
    ":label: eqn-preconditioned-regression\n",
    "\\min_{\\vec{y}} \\|\\vec{A}\\vec{M}\\vec{y} - \\vec{b}\\|\n",
    ",\\quad\n",
    "\\vec{x} = \\vec{M}\\vec{y}.\n",
    "```\n",
    "The rate of convergence of iterative methods applied to the preconditioned system depends on the condition number of the preconditioned matrix $\\vec{A}\\vec{M}$.\n",
    "\n",
    ":::{prf:theorem}\n",
    "\n",
    "Suppose LSQR[^lsqr] is applied to {ref}`eqn-preconditioned-regression` for $t$ iterations from initial guess $\\vec{x}_0$ to produce an approximate solution $\\vec{x}_t$.\n",
    "Then, for some \n",
    "\\begin{equation*}\n",
    "t = O \\left( \\cond(\\vec{A}\\vec{M}) \\log \\left( \\frac{1}{\\varepsilon} \\right) \\right),\n",
    "\\end{equation*}\n",
    "it holds that\n",
    "\\begin{equation*}\n",
    "\\| \\vec{A}(\\vec{x}^* - \\vec{x}_t) \\| \\leq \\varepsilon \\|\\vec{A}(\\vec{x}^* - \\vec{x}_0)\\|.\n",
    "\\end{equation*}\n",
    "This requires $t$ matrix-vector products with $\\vec{A}$, $\\vec{A}^\\T$, $\\vec{M}$ and $\\vec{M}^\\T$ and $O(tn)$ additional flops.\n",
    ":::\n",
    "\n",
    "[^lsqr]: LSQR is mathematically equivalent to the conjugate gradient method applied to the normal equations, but it is more numerically stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Preconditioning\n",
    "\n",
    "The basic sketch-and-precondition algorithm follows a two-stage approach: first construct a preconditioner using a randomized sketch, then solve the preconditioned system using an iterative method run for $t$ iterations.\n",
    "\n",
    ":::{prf:algorithm} Sketch and Precondition\n",
    ":label: sketch-and-precondition\n",
    "\n",
    "**Input:** $\\vec{A}\\in\\R^{n\\times d}$, $\\vec{b}\\in\\R^n$, sketching dimension $k$, tolerance $\\varepsilon$\n",
    "\n",
    "1. Get $\\_,\\vec{R} = \\Call{Sketched-QR}(\\vec{A},k)$\n",
    "1. Form preconditioner $\\vec{M} = \\vec{R}^{-1}$\n",
    "1. Get initial guess $\\vec{x}_0$ from sketch-and-solve\n",
    "1. Set $\\widehat{\\vec{x}} = \\Call{Iterative-Method}_t(\\vec{A},\\vec{b},\\vec{x}_0,\\vec{M})$\n",
    "\n",
    "**Output:** $\\widehat{\\vec{x}}$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Any version of the [sketched-QR algorithm](../03-QR-Factorization/randomized-cholesky-qr.ipynb) that produces subspace embedding will yield a high-quality preconditioner.\n",
    "\n",
    ":::{prf:theorem}\n",
    "\n",
    "Suppose $\\vec{A}$ is full-rank and the sketch $\\vec{S}$ used by the [sketched-QR algorithm](../03-QR-Factorization/randomized-cholesky-qr.ipynb) is an $\\varepsilon$-subspace embedding for $\\vec{A}$.\n",
    "Let $\\vec{M} =\\vec{R}^{-1}$, where $\\vec{R}$ is the upper-triangular factor returned by the [sketched-QR algorithm](../03-QR-Factorization/randomized-cholesky-qr.ipynb).\n",
    "Then \n",
    "\\begin{equation*}\n",
    "\\cond(\\vec{A}\\vec{M}) \\leq  \\frac{1+\\varepsilon}{1-\\varepsilon}.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    ":::{prf:proof}\n",
    ":class: dropdown\n",
    ":enumerated: false\n",
    "\n",
    "The [sketched-QR algorithm](../03-QR-Factorization/randomized-cholesky-qr.ipynb) gives a factorization $\\vec{A} = \\vec{Q}\\vec{R}$.\n",
    "By {eq}`sketched-qr-well-conditioned`,\n",
    "\\begin{equation*}\n",
    "\\cond(\\vec{A}\\vec{M}) = \\cond(\\vec{A}\\vec{R}^{-1}) = \\cond(\\vec{Q}) \\leq (1+\\varepsilon)/(1-\\varepsilon)\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    "\n",
    "Note that we initialized the iterative method from the sketch-and-solve solution.\n",
    "This ensures that the initial error is bounded. \n",
    "\n",
    "Together, the above observations yield the following guarantee.\n",
    ":::{prf:theorem}\n",
    "Let $\\widehat{\\vec{x}}$ be the output of {prf:ref}`sketch-and-precondition` with a sketch that is an $\\varepsilon$-subspace embedding for $\\vec{A}$ with $\\varepsilon \\leq 1/2$.\n",
    "If LSQR is used as the iterative method, then for some \n",
    "\\begin{equation*}\n",
    "t = O \\left( \\log \\left( \\frac{1}{\\varepsilon} \\right) \\right),\n",
    "\\end{equation*}\n",
    "it holds that\n",
    "\\begin{equation*}\n",
    "\\| \\vec{A}(\\vec{x}^* - \\vec{x}_t) \\| \\leq \\varepsilon \\|\\vec{b} - \\vec{A}\\vec{x}\\|\n",
    "\\leq \\varepsilon \\|\\vec{b}\\|.\n",
    "\\end{equation*}\n",
    "This requires $t$ matrix-vector products with $\\vec{A}$, $\\vec{A}^\\T$, $\\vec{M}$ and $\\vec{M}^\\T$ and $O(tn)$ additional flops.\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "Sketch and precondition can be implemented in Numpy as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketch_and_precondition(A,b,k,zeta,n_iters,rng):\n",
    "    \n",
    "    n, d = A.shape\n",
    "    S = sparse_stack_sketch(n,k,zeta,rng) \n",
    "    Y = S @ A \n",
    "    Q,R = np.linalg.qr(Y, mode='reduced')\n",
    "\n",
    "    x0 = sp.linalg.solve_triangular(R,Q.T@(S@b),lower=False)\n",
    "\n",
    "    M = sp.linalg.solve_triangular(R,np.eye(d),lower=False)\n",
    "\n",
    "    x = lsqr(A,b,x0,M,n_iters)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Numerical Experiment\n",
    "\n",
    "We now perform  numerical experiment to illustrate the behavior of sketch-and-precondition. \n",
    "Here we solve an ill-conditioned regression problem and compare the convergence of sketch-and-precondition for several choices of embedding dimension. \n",
    "Using a large embedding dimension leads to a better preconditioner and faster convergence, but requires some additional time to compute the preconditioner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from randnla import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random matrix with controlled condition number\n",
    "n = 100_000\n",
    "d = 800\n",
    "cond = 1e8\n",
    "\n",
    "U, s, Vt = np.linalg.svd(np.random.rand(n,d),full_matrices=False)\n",
    "s = np.geomspace(1/cond,1,d)  # define singular values\n",
    "A = U@np.diag(s)@Vt\n",
    "\n",
    "# define ||b|| and ||b-Ax*||\n",
    "b_norm = 1\n",
    "residual_norm = .1\n",
    "\n",
    "# construct right hand side\n",
    "v = np.random.randn(n)\n",
    "v_span = U @(U.T@v)\n",
    "v_perp = v - v_span\n",
    "v_span /= np.linalg.norm(v_span)\n",
    "v_perp /= np.linalg.norm(v_perp)\n",
    "\n",
    "b = v_span*np.sqrt(b_norm**2-residual_norm**2)+v_perp*residual_norm\n",
    "\n",
    "# construct true solution\n",
    "x_true = Vt.T@(np.diag(1/s)@(U.T@b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experimental configurations\n",
    "zeta = 4\n",
    "rng = np.random.default_rng(0) \n",
    "\n",
    "experiment_configs = [\n",
    "    {'k': 2*d, 'iters': np.linspace(0,40,5,dtype='int')},\n",
    "    {'k': 4*d, 'iters': np.linspace(0,23,5,dtype='int')},\n",
    "    {'k': 12*d, 'iters': np.linspace(0,13,5,dtype='int')},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "n_repeat = 10\n",
    "errs = {}\n",
    "times = {}\n",
    "\n",
    "for experiment in experiment_configs:\n",
    "    k = experiment['k']\n",
    "    iters = experiment['iters']\n",
    "\n",
    "    for n_iters in iters:\n",
    "\n",
    "        start = time.time()\n",
    "        for _ in range(n_repeat):\n",
    "            x = sketch_and_precondition(A,b,k,zeta,n_iters,rng)\n",
    "        end = time.time()\n",
    "        avg_time = (end - start) / n_repeat\n",
    "        \n",
    "        errs[k,n_iters] = np.linalg.norm(x-x_true)/np.linalg.norm(x_true)\n",
    "        times[k,n_iters] = avg_time\n",
    "\n",
    "# Time baseline method\n",
    "start = time.time()\n",
    "for _ in range(n_repeat):\n",
    "    x_np = np.linalg.lstsq(A,b,rcond=None)[0]\n",
    "end = time.time()\n",
    "\n",
    "avg_time_np = (end - start) / n_repeat\n",
    "err_np = np.linalg.norm(A@(x_np-x_true))/np.linalg.norm(A@x_true)\n",
    "\n",
    "# Plot the results\n",
    "fig, axs = plt.subplots(1,2,figsize=(10, 4),sharey=True)\n",
    "\n",
    "for experiment in experiment_configs:\n",
    "    k = experiment['k']\n",
    "    iters = experiment['iters']\n",
    "    \n",
    "    times_k = [times[k,n_iters] for n_iters in iters]\n",
    "    errs_k = [errs[k,n_iters] for n_iters in iters]\n",
    "\n",
    "    axs[0].plot(iters,errs_k,marker='o',label=f'$k={k//d}d$')\n",
    "    axs[1].plot(times_k,errs_k,marker='o',label=f'$k={k//d}d$')\n",
    "\n",
    "axs[1].scatter(avg_time_np,err_np,marker='x',color='k',label='Numpy')\n",
    "\n",
    "axs[0].set_xlabel('iterations')\n",
    "axs[1].set_xlabel('time (s)')\n",
    "\n",
    "axs[0].set_ylabel('error')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.savefig('sketch_and_precondition.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two-panel comparison plot for sketch-and-precondition performance. Left panel shows error vs iterations for different sketch sizes (k=2d, 4d, 12d), demonstrating faster convergence with larger sketches. Right panel shows error vs computation time, comparing sketch-and-precondition methods to NumPy's direct solver (marked with x). All methods achieve better time-to-accuracy trade-offs than the direct method.](./sketch_and_precondition.svg)\n",
    "\n",
    "Even when targeting full-accuracy, sketch and precondition outperforms Numpy.\n",
    "In settings where moderate accuracy is sufficient or where $\\vec{A}$ is sparse, the speedups may be even more substantial!\n",
    "\n",
    "Note also that as the embedding dimension $k$ increases, the quality of the preconditioner improves, leading to faster convergence of the iterative method.\n",
    "However, this also increases the time required to compute the preconditioner.\n",
    "A good implementation will balance these trade-offs to achieve the best overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter selection\n",
    "\n",
    "For sketches that behave similarly to Gaussian sketches, we can use the [Gaussian theory](../02-Sketching/Gaussian-sketch.ipynb/#spectral-properties) to estimate the largest and smallest eigenvalues of of the preconditioned system $\\vec{A}\\vec{M}$ as a function of the embedding dimension $k$.\n",
    "\n",
    "This can be used in a variety of ways. \n",
    "For example:\n",
    "- To get an a priori estimate of the number of iterations required to reach a desired accuracy.\n",
    "    - This can then be used to balance the time spent computing the preconditioner (increases with $k$) vs the time spent in the iterative method (decreases with $k$).\n",
    "- To use a iterative method like heavy ball momentum which requires knowledge of the extreme eigenvalues to set parameters, but may be more parallelize than LSQR.\n",
    "\n",
    "Further discussion can be found in {cite:p}`chen_niroula_ray_subrahmanya_pistoia_kumar_25` and the references within.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
