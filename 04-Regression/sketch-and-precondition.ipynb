{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Sketch and Precondition\n",
    "description: High-accuracy linear regression using randomized sketching for preconditioning with detailed coverage of preconditioned iterative methods\n",
    "keywords: [sketch and precondition, preconditioning, LSQR, condition number, sketched QR, convergence acceleration, randomized preconditioning]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 4.%s\n",
    "    continue: true\n",
    "---\n",
    "\n",
    "The sketch-and-precondition approach provides a powerful alternative to both direct factorization methods and the [sketch-and-solve](sketch-and-solve.ipynb) paradigm for solving {eq}`task-regression`.\n",
    "Unlike sketch-and-solve, which may sacrifice accuracy for speed, sketch-and-precondition maintains high accuracy while achieving significant computational speedups.\n",
    "\n",
    "The key insight behind sketch-and-precondition is to use a randomized sketch to construct a preconditioner that improves the conditioning the {eq}`task-regression`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Preconditioned Iterative Methods\n",
    "\n",
    "Given any invertivle matrix $\\vec{M}\\in\\R^{d\\times d}$ (called a *preconditioner*), solving {eq}`task-regression` is equivalent to solving\n",
    "```{math}\n",
    ":label: eqn-preconditioned-regression\n",
    "\\min_{\\vec{y}} \\|\\vec{A}\\vec{M}\\vec{y} - \\vec{b}\\|\n",
    ",\\quad\n",
    "\\vec{x} = \\vec{M}\\vec{y}.\n",
    "```\n",
    "The convergence of iterative methods applied to the preconditioned system depends on the condition number of the preconditioned matrix $\\vec{A}\\vec{M}$.\n",
    "\n",
    ":::{prf:theorem}\n",
    "\n",
    "Suppose LSQR is applied to {ref}`eqn-preconditioned-regression` for $t$ iterations from initial guess $\\vec{x}_0$ to produce an approximate solution $\\vec{x}_t$.\n",
    "Then, for some \n",
    "\\begin{equation*}\n",
    "t = O \\left( \\cond(\\vec{A}\\vec{M}) \\log \\left( \\frac{1}{\\varepsilon} \\right) \\right),\n",
    "\\end{equation*}\n",
    "it holds that\n",
    "\\begin{equation*}\n",
    "\\| \\vec{A}(\\vec{x}^* - \\vec{x}_t) \\| \\leq \\varepsilon \\|\\vec{A}(\\vec{x}^* - \\vec{x}_0)\\|.\n",
    "\\end{equation*}\n",
    "This requires $t$ matrix-vector products with $\\vec{A}$, $\\vec{A}^\\T$, $\\vec{M}$ and $\\vec{M}^\\T$, in addition to $O(tn)$ flops.\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Preconditioning\n",
    "\n",
    "The basic sketch-and-precondition algorithm follows a two-stage approach: first construct a preconditioner using a randomized sketch, then solve the preconditioned system using an iterative method.\n",
    "\n",
    ":::{prf:algorithm} Sketch and Precondition\n",
    ":label: sketch-and-precondition\n",
    "\n",
    "**Input:** $\\vec{A}\\in\\R^{n\\times d}$, $\\vec{b}\\in\\R^n$, sketching dimension $k$, tolerance $\\varepsilon$\n",
    "\n",
    "1. Get $\\vec{Q}_1,\\vec{R}_1 = \\Call{Sketched-QR}(\\vec{A},k)$\n",
    "1. Form preconditioner $\\vec{M} = \\vec{R}^{-1}$\n",
    "1. Get initial guess $\\vec{x}_0$ from sketch-and-solve\n",
    "1. Set $\\widehat{\\vec{x}} = \\Call{Iterative-Method}(\\vec{A},\\vec{b},\\vec{x}_0,\\vec{M})$\n",
    "\n",
    "**Output:** $\\vec{x}$\n",
    ":::\n",
    "\n",
    "The role of the sketch-and-solve initial guess is to ensure that the initial error is bounded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketch_and_precondition(A,b,k,zeta,n_iters,rng):\n",
    "    \n",
    "    n, d = A.shape\n",
    "    S = sparse_stack_sketch(n,k,zeta,rng) \n",
    "    Y = S @ A \n",
    "    Q,R = np.linalg.qr(Y, mode='reduced')\n",
    "\n",
    "    x0 = sp.linalg.solve_triangular(R,Q.T@(S@b),lower=False)\n",
    "\n",
    "    M = sp.linalg.solve_triangular(R,np.eye(d),lower=False)\n",
    "\n",
    "    x = lsqr(A,b,x0,M,n_iters)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Any version of [Sketched-QR algorithm](../03-QR-Factorization/randomized-cholesky-qr.ipynb) that produces subspace embedding can be used.\n",
    "\n",
    ":::{prf:theorem}\n",
    "\n",
    "Suppose $\\vec{A}$ is full-rank and the sketch $\\vec{S}$ used by the [Sketched-QR algorithm](../03-QR-Factorization/randomized-cholesky-qr.ipynb) is an $\\varepsilon$-subspace embedding for $\\vec{A}$.\n",
    "Then \n",
    "\\begin{equation*}\n",
    "\\cond(\\vec{A}\\vec{M}) \\leq  \\frac{1+\\varepsilon}{1-\\varepsilon}.\n",
    "\\end{equation*}\n",
    ":::\n",
    "\n",
    ":::{prf:proof}\n",
    ":class: dropdown\n",
    ":enumerated: false\n",
    "\n",
    "Recall $\\vec{A} = \\vec{Q}\\vec{R}$.\n",
    "By {eq}`sketched-qr-well-conditioned`,\n",
    "\\begin{equation*}\n",
    "\\cond(\\vec{A}\\vec{M}) = \\cond(\\vec{A}\\vec{R}^{-1}) = \\cond(\\vec{Q}) \\leq (1+\\varepsilon)/(1-\\varepsilon)\n",
    "\\end{equation*}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Numerical Experiment\n",
    "\n",
    "We now perform  numerical experiment to illustrate the behavior of sketch-and-precondition. \n",
    "Here we solve an ill-conditioned regression problem and compare the convergence of sketch-and-precondition for several choices of embedding dimension. \n",
    "Using a large embedding dimension leads to a better preconditioner and faster convergence, but requires some additional time to compute the preconditioner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from randnla import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Set up the test problem\n",
    "# ========================\n",
    "n = 100_000\n",
    "d = 800\n",
    "cond = 1e8\n",
    "\n",
    "\n",
    "U, s, Vt = np.linalg.svd(np.random.rand(n, d), full_matrices=False)\n",
    "s = np.geomspace(1/cond, 1, d)  # Controlled singular values for numerical stability\n",
    "A = U @ np.diag(s) @ Vt\n",
    "\n",
    "b_norm = 1\n",
    "residual_norm = .1\n",
    "\n",
    "v = np.random.randn(n)\n",
    "v_span = U @ (U.T @ v)\n",
    "v_perp = v - v_span\n",
    "v_span /= np.linalg.norm(v_span)\n",
    "v_perp /= np.linalg.norm(v_perp)\n",
    "\n",
    "b = v_span * np.sqrt(b_norm**2 - residual_norm**2) + v_perp * residual_norm\n",
    "\n",
    "x_true = Vt.T@(np.diag(1/s)@(U.T@b))\n",
    "\n",
    "# ========================\n",
    "# Now run the experiment\n",
    "# ========================\n",
    "\n",
    "zeta = 4\n",
    "rng = np.random.default_rng(0) \n",
    "\n",
    "experiments = [\n",
    "    {'k': 2*d,'iters': np.linspace(0,40,5,dtype='int')},\n",
    "    {'k': 4*d,'iters': np.linspace(0,23,5,dtype='int')},\n",
    "    {'k': 12*d,'iters': np.linspace(0,13,5,dtype='int')},\n",
    "]\n",
    "\n",
    "n_repeat = 10\n",
    "errs = {}\n",
    "times = {}\n",
    "\n",
    "for experiment in experiments:\n",
    "    k = experiment['k']\n",
    "    iters = experiment['iters']\n",
    "\n",
    "    for n_iters in iters:\n",
    "\n",
    "        start = time.time()\n",
    "        for _ in range(n_repeat):\n",
    "            x = sketch_and_precondition(A,b,k,zeta,n_iters,rng)\n",
    "        end = time.time()\n",
    "        avg_time = (end - start) / n_repeat\n",
    "        \n",
    "        errs[k,n_iters] = np.linalg.norm(x-x_true)/np.linalg.norm(x_true)\n",
    "        times[k,n_iters] = avg_time\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(n_repeat):\n",
    "    x_np = np.linalg.lstsq(A,b,rcond=None)[0]\n",
    "end = time.time()\n",
    "\n",
    "avg_time_np = (end - start) / n_repeat\n",
    "err_np = np.linalg.norm(A@(x_np-x_true))/np.linalg.norm(A@x_true)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Plot the results\n",
    "# ========================\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(10, 4),sharey=True)\n",
    "\n",
    "for experiment in experiments:\n",
    "    k = experiment['k']\n",
    "    iters = experiment['iters']\n",
    "    \n",
    "    times_k = [times[k,n_iters] for n_iters in iters]\n",
    "    errs_k = [errs[k,n_iters] for n_iters in iters]\n",
    "\n",
    "    axs[0].plot(iters,errs_k,marker='o',label=f'$k={k//d}d$')\n",
    "    axs[1].plot(times_k,errs_k,marker='o',label=f'$k={k//d}d$')\n",
    "\n",
    "axs[1].scatter(avg_time_np,err_np,marker='x',color='k',label='Numpy')\n",
    "\n",
    "axs[0].set_xlabel('iterations')\n",
    "axs[1].set_xlabel('time (s)')\n",
    "\n",
    "axs[0].set_ylabel('error')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.savefig('sketch_and_precondition.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "![Two-panel comparison plot for sketch-and-precondition performance. Left panel shows error vs iterations for different sketch sizes (k=2d, 4d, 12d), demonstrating faster convergence with larger sketches. Right panel shows error vs computation time, comparing sketch-and-precondition methods to NumPy's direct solver (marked with x). All methods achieve better time-to-accuracy trade-offs than the direct method.](./sketch_and_precondition.svg)\n\nEven when targeting full-accuracy, sketch and precondition outperforms Numpy.\nIn settings where moderate accuracy is sufficient or where $\\vec{A}$ is sparse, the speedups may be even more substantial!\n\nNote also that as the embedding dimension $k$ increases, the quality of the preconditioner improves, leading to faster convergence of the iterative method.\nHowever, this also increases the time required to compute the preconditioner.\nA good implementation will balance these trade-offs to achieve the best overall performance."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}