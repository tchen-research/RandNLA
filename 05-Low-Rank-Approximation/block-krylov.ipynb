{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Block Krylov Iteration\n",
    "description: Advanced techniques for improved accuracy and robustness in randomized low-rank approximation\n",
    "keywords: [subspace iteration, block Krylov, power iteration, Krylov subspace, iterative methods, spectral gap, convergence acceleration]\n",
    "numbering:\n",
    "  equation:\n",
    "    enumerator: 5.%s\n",
    "    continue: true\n",
    "  proof:theorem:\n",
    "    enumerator: 5.%s\n",
    "    continue: true\n",
    "  proof:algorithm:\n",
    "    enumerator: 5.%s\n",
    "    continue: true\n",
    "  proof:definition:\n",
    "    enumerator: 5.%s\n",
    "    continue: true\n",
    "  proof:proposition:\n",
    "    enumerator: 5.%s\n",
    "    continue: true\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Subspace Iteration](./subspace-iteration.ipynb) can dramatically improve on the performance of the [Randomized SVD](./randomized-svd.ipynb) when $\\vec{A}$ has a heavy tail. \n",
    "The key observation is that the singular value tail of $(\\vec{A}\\vec{A}^\\T)^q\\vec{A}$ is much lighter than that of $\\vec{A}$.\n",
    "This is because the polynomial $x^{2q+1}$ pushes tail singular values of $\\vec{A}$ to zero.\n",
    "\n",
    "From a computational perspective, there isn't that much special about $x^{2q+1}$. In fact, we can replace this monomial with any polynomial of the same degree containing only odd degree terms, without significantly changing the cost of the algorithm.\n",
    "Thus, we might consider an approximation of the form:\n",
    "```{math}\n",
    ":label: eqn-arbitrary-poly-lra\n",
    "\\widehat{\\vec{A}} = \\vec{Q}\\vec{Q}^\\T \\vec{A},\n",
    "\\quad \n",
    "\\vec{Q} = \\Call{orth}(p(\\vec{A}\\vec{A}^\\T)\\vec{A}\\vec{\\Omega})\n",
    ",\\quad \\deg(p) \\leq q.\n",
    "```\n",
    "It's conceivable that, if we choose a good polynomial, this method could be more accurate than the subspace iteration. \n",
    "For instance, Chebyshev polynomials grow substantially faster than monomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Block Krylov Iteration\n",
    "\n",
    "The difficulty with the approach in [](#eqn-arbitrary-poly-lra) is that we don't know how to choose a good polynomial in advance, since we don't know the singular values of $\\vec{A}$.\n",
    "\n",
    "Observe that for any polynomial $p(x)$ with $\\deg(p)<t$,\n",
    "\\begin{equation}\n",
    "p(\\vec{A}\\vec{A}^\\T)\\vec{A}\\vec{\\Omega} \\in \\mathcal{K}_t(\\vec{A}\\vec{A}^\\T,\\vec{A}\\vec{\\Omega}),\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\mathcal{K}_t(\\vec{A}\\vec{A}^\\T,\\vec{A}\\vec{\\Omega}) := \\operatorname{span}\\left\\{ \\vec{A}\\vec{\\Omega}, (\\vec{A}\\vec{A}^\\T)\\vec{A}\\vec{\\Omega}, \\ldots, (\\vec{A}\\vec{A}^\\T)^{t-1}\\vec{A}\\vec{\\Omega} \\right\\}.\n",
    "\\end{equation}\n",
    "In particular, note that, using the same number of matrix-vector products with $\\vec{A}$ and $\\vec{A}^\\T$ as required to compute $p(\\vec{A}\\vec{A}^\\T)\\vec{A}\\vec{\\Omega}$, we can actually compute an orthonormal basis for the entire block Krylov subspace $\\mathcal{K}_t(\\vec{A}\\vec{A}^\\T,\\vec{A}\\vec{\\Omega})$.\n",
    "\n",
    "This suggests the *Randomized Block Krylov Iteration* (RBKI) approximation:\n",
    "\\begin{equation}\n",
    "\\widehat{\\vec{A}} = \\vec{Q}\\vec{Q}^\\T \\vec{A},\n",
    "\\quad \n",
    "\\vec{Q} = \\Call{orth}(\\mathcal{K}_t(\\vec{A}\\vec{A}^\\T,\\vec{A}\\vec{\\Omega})).\n",
    "\\end{equation}\n",
    "Intuitively, by projecting onto a larger space, we can get a better approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Block-size versus depth\n",
    "\n",
    "The orthonormal basis for $\\vec{A}\\mathcal{K}_t(\\vec{A}\\vec{A}^\\T,\\vec{A}\\vec{\\Omega})$ has up to $tb$ columns, where $b$ is the number of columns in $\\vec{\\Omega}$, so we may hope to obtain a good rank-$k$ approximation even if $b<k$.\n",
    "However, this raises the practical question of how the block-size should be set.\n",
    "We tend to observe two competing factors:\n",
    "- **Smaller is better:**\n",
    "While the worst-case matrix-vector product complexity for block sizes $b=1$ and $b=k$ match, the convergence of Krylov methods on a given instance $\\vec{A}$ is highly dependent on $\\vec{A}$'s spectrum, so \"better-than-worst-case\" performance is often observed. Theoretical evidence suggests that, in the absence of small singular value gaps, choosing $b=1$ typically requires fewer matrix-vector products to reach a desired level of accuracy for non-worst-case instances; see {cite:p}`meyer_musco_musco_24`.\n",
    "- **Bigger is better:** Due to parallelism, caching, and dedicated hardware accelerators in modern computing systems, it is typically much faster to compute multiple matrix-vector products all at once (i.e., grouped into a matrix-matrix product) instead of one after the other. \n",
    "This was observed in our experiments on [the cost of NLA](../01-Background/cost-of-numerical-linear-algebra.ipynb).\n",
    "RBKI groups matrix-vector products into blocks of size $b$, so for a fixed number of total matrix-vector products, we expect the method to run faster if $b$ is larger. \n",
    "The benefits of blocking into matrix-matrix products can be even more pronounced when $\\vec{A}$ is too large to store in fast memory, in which case the cost of reading $\\vec{A}$ from slow memory is a bottleneck, and using a larger block size $b\\gg 1$ can be essentially \"free\".\n",
    "\n",
    "These two effects are in competition with one another, and one observes (e.g. as in the experiment below) that the best block size $b$ is often $1\\ll b\\ll k$.\n",
    "\n",
    "\n",
    "### Convergence Guarantees\n",
    "\n",
    "<!-- RBKI with $\\sqrt{q}$ iterations can achieve similar guarantees to subspace iteration with $q$ iterations.\n",
    "This was first proved in {cite:p}`musco_musco_15`; see also {cite:p}`tropp_webber_23` for a non-asymptotic analysis. -->\n",
    "\n",
    "Theoretical work answer this question is provided in {cite:p}`chen_epperly_meyer_musco_rao_25`, which builds off of {cite:p}`meyer_musco_musco_24`.\n",
    "In particular, {cite:p}`chen_epperly_meyer_musco_rao_25` asserts that, up to logartihmic factors in $n$ and certain spectral properties of $\\vec{A}$, the number of matrix-vector products required to obtain a near-optimal approximation can be bounded independent of the block-size.\n",
    "\n",
    ":::{prf:theorem} \n",
    "Let $\\widehat{\\vec{A}}$ be the rank-$k$ approximation to $\\vec{A}$ produced by RBKI after $t$ iterations. \n",
    "Then for some\n",
    "\\begin{equation*}\n",
    "t = \\tilde{O}\\left( \\frac{k/b}{\\sqrt{\\varepsilon}}  \\right).\n",
    "\\end{equation*}\n",
    "it holds that\n",
    "\\begin{equation*}\n",
    "\\|\\vec{A} - \\llbracket \\widehat{\\vec{A}}\\rrbracket_k \\| \\leq (1+\\varepsilon) \\|\\vec{A} - \\llbracket \\vec{A} \\rrbracket_k\\|.\n",
    "\\end{equation*}\n",
    "::: \n",
    "\n",
    "This means that users are free to choose the block-size based on their computational environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Numerical Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from randnla import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# load experiment\n",
    "\n",
    "n = 4000\n",
    "name = 'intro'\n",
    "Λ = np.geomspace(1,100,n)\n",
    "blocksizes = [1,5,20,100,200]\n",
    "qs = [650,200,60,11,7]\n",
    "skips = [20,4,1,1,1]\n",
    "k = 100\n",
    "\n",
    "A = np.diag(Λ)\n",
    "err_opt = np.linalg.norm(np.sort(Λ)[:-k])\n",
    "\n",
    "class AAt:\n",
    "    def __matmul__(self,X):\n",
    "        return A@(A.T@X)\n",
    "        \n",
    "AAt = AAt()\n",
    "\n",
    "# run experiment\n",
    "all_times = {}\n",
    "all_errs = {}\n",
    "for i,(b,q,skip) in enumerate(zip(blocksizes,qs,skips)):\n",
    "\n",
    "    B = np.random.randn(n,b)\n",
    "    \n",
    "    Q,_,times = block_lanczos(AAt,B,q,reorth=True)\n",
    "\n",
    "    all_times[b] = times\n",
    "\n",
    "    all_errs[b] = np.zeros(q+1)\n",
    "        \n",
    "    for j in range(0,q+1,skip):\n",
    "\n",
    "        Zj = Q[:,:b*j]\n",
    "\n",
    "        X = Zj.T@A\n",
    "        Qtilde,s,Vt = np.linalg.svd(X,full_matrices=False)\n",
    "\n",
    "            \n",
    "        A_hat = Zj@(Qtilde[:,:k]@(np.diag(s[:k])@Vt[:k]))\n",
    "        err = np.linalg.norm(A - A_hat)\n",
    "        \n",
    "        all_errs[b][j] = err\n",
    "\n",
    "# plot the error\n",
    "fig, axs = plt.subplots(1,2,figsize=(8, 4),sharey=True)\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "for i,(b,q,skip) in enumerate(zip(blocksizes,qs,skips)):\n",
    "\n",
    "    errs = all_errs[b]\n",
    "    times = all_times[b]\n",
    "    \n",
    "    ax = axs[0]\n",
    "\n",
    "    ax.set_ylabel(r'accuracy $\\varepsilon$')\n",
    "\n",
    "    ax.plot(np.arange(0,q+1)[::skip]*b,errs[::skip]/err_opt-1,ms=5,label=f'$b={b}$')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(1e-6,1e0)\n",
    "    ax.set_xlabel('matrix-vector products')\n",
    "      \n",
    "    ax = axs[1]\n",
    "    \n",
    "    ax.plot(times[::skip],errs[::skip]/err_opt-1,ms=5,label=f'$b={b}$')\n",
    "    ax.set_xlabel('wall-clock time (s)')\n",
    "    ax.set_xlim(-.1,3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.savefig(f'RBKI.svg')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the value of $\\varepsilon$ so that $\\|\\vec{A} -\\llbracket \\widehat{\\vec{A}}\\rrbracket_k  \\|_\\F \\leq (1+\\varepsilon) \\|\\vec{A} - \\llbracket \\vec{A} \\rrbracket_k\\|_\\F$.\n",
    "\n",
    "\n",
    "![](RBKI.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
