{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\ntitle: The Cost of Numerical Linear Algebra\ndescription: Overview of measuring computational costs in numerical linear algebra including floating point operations, matrix-vector queries, and entry evaluations\nkeywords: [computational cost, floating point operations, flops, matrix-vector multiplication, precision, hardware efficiency, numerical linear algebra]\n---\n\nRandomized Numerical Linear Algebra (RandNLA) achieves speedups over classical NLA algorithms is by one or both of the following strategies:\n1. reducing the complexity of algorithms\n1. reorganizing computation to take advantage of modern hardware\n\n\nWhile practitioners may care about particular quantities like runtime, energy use, or monetary cost, these metrics are highly system dependent.\nAs such, it is beneficial to understand the underlying costs of algorithms in a more abstract way.\nIn this section, we provide a brief overview some common ways of measuring costs in NLA, which will be useful for understanding the improvements that RandNLA can provide.\nOf course, there is no \"best\" way of measuring costs, and"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating point operations\n",
    "\n",
    "Perhaps the most common way of measuring the cost of numerical algorithms is by counting the number of floating point operations (flops) required to complete the algorithm.\n",
    "This provides a mathematical (hardware-independent) measure of algorithmic complexity, which is useful for comparing algorithms and understanding how they scale with the input size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flops are not necessarily representative of cost\n",
    "\n",
    "Flops alone don't tell us everything, since the flop rate (number of flops per unit time) can vary significantly depending on the specific operation being performed and the hardware being used.\n",
    "On modern computing environments, other factors such as memory access patterns, communication costs, and parallelism can have a significant impact on the actual runtime of an algorithm.\n",
    "In fact, some modern hardware devices are literally designed to perform certain types of linear-algebra operations (e.g., matrix-matrix multiplication) very efficiently (see e.g. [nVidia's Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To illustrate this point, let's consider the following basic linear-algebra operations:\n",
    "1. matrix-vector multiplication: $\\vec{A}\\vec{x}$ \n",
    "2. matrix-matrix multiplication: $\\vec{A}^\\T\\vec{A}$\n",
    "3. QR factorization of $\\vec{A}$\n",
    "\n",
    "\n",
    "We'll time each of these operations on a $n\\times d$ matrix $\\vec{A}$.By dividing the time by the number of flops, we can get a sense of relative efficiency of these primitives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random matrix and vector\n",
    "n = 2000\n",
    "d = 500\n",
    "\n",
    "A = np.random.randn(n,d)\n",
    "x = np.random.randn(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the operations and their flop counts\n",
    "operations = {\n",
    "    'A@x': {\n",
    "        'func': lambda: A@x,\n",
    "        'flops': 2 * n * d\n",
    "    },\n",
    "    'A.T@A': {\n",
    "        'func': lambda: A.T@A,\n",
    "        'flops': 2 * n * d**2\n",
    "    },\n",
    "    'QR(A)': {\n",
    "        'func': lambda: np.linalg.qr(A),\n",
    "        'flops': 2 * n * d**2 - (2/3) * d**3\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now time each of these operations and compare the flop rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5e6cb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5e6cb_level0_col0\" class=\"col_heading level0 col0\" >method</th>\n",
       "      <th id=\"T_5e6cb_level0_col1\" class=\"col_heading level0 col1\" >flops/s</th>\n",
       "      <th id=\"T_5e6cb_level0_col2\" class=\"col_heading level0 col2\" >flops/s (relative)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5e6cb_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5e6cb_row0_col0\" class=\"data row0 col0\" >A@x</td>\n",
       "      <td id=\"T_5e6cb_row0_col1\" class=\"data row0 col1\" >6.0e+08</td>\n",
       "      <td id=\"T_5e6cb_row0_col2\" class=\"data row0 col2\" >0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e6cb_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5e6cb_row1_col0\" class=\"data row1 col0\" >A.T@A</td>\n",
       "      <td id=\"T_5e6cb_row1_col1\" class=\"data row1 col1\" >1.3e+11</td>\n",
       "      <td id=\"T_5e6cb_row1_col2\" class=\"data row1 col2\" >100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e6cb_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5e6cb_row2_col0\" class=\"data row2 col0\" >QR(A)</td>\n",
       "      <td id=\"T_5e6cb_row2_col1\" class=\"data row2 col1\" >1.1e+10</td>\n",
       "      <td id=\"T_5e6cb_row2_col2\" class=\"data row2 col2\" >8%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7afbbd8bcf10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time the operations\n",
    "n_repeat = 20 # Number of repetitions for averaging\n",
    "\n",
    "results = [] # Initialize results list\n",
    "\n",
    "# Time each operation\n",
    "for method_name, operation in operations.items():\n",
    "    start = time.time()\n",
    "    for _ in range(n_repeat):\n",
    "        operation['func']()\n",
    "    end = time.time()\n",
    "    \n",
    "    avg_time = (end - start) / n_repeat\n",
    "    \n",
    "    results.append({\n",
    "        'method': method_name,\n",
    "        'flops/s': operation['flops']/avg_time \n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['flops/s (relative)'] = 100*results_df['flops/s'] / results_df['flops/s'].max()\n",
    "results_df.style.format({'flops/s':'{:1.1e}','flops/s (relative)':'{:1.0f}%'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our experiment are striking. Even these basic linear algebra operations differ in efficinecy by orders of magnitude.\n",
    "For instance, the flop rate of matrix-vector multiplication is about 10x lower than that of matrix-matrix multiplication.\n",
    "This means that   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision impacts cost\n",
    "\n",
    "While NLA has traditionally been performed in double precision, many modern hardware devices (e.g., GPUs) are optimized for single precision or even lower precision arithmetic. \n",
    "Perhaps unsurprisingly, using lower precision can substantially increase the flop rate of numerical linear algebra.\n",
    "\n",
    "To illusrate this, we compare the flop rates of matrix-matrix multiplication in single and double precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices in different precisions\n",
    "A_f64 = np.random.randn(n, d).astype(np.float64)\n",
    "A_f32 = A_f64.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, we'll benchmark the flop rates of matrix-matrix multiplication in single and double precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5896f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5896f_level0_col0\" class=\"col_heading level0 col0\" >method</th>\n",
       "      <th id=\"T_5896f_level0_col1\" class=\"col_heading level0 col1\" >time (s)</th>\n",
       "      <th id=\"T_5896f_level0_col2\" class=\"col_heading level0 col2\" >flops/s</th>\n",
       "      <th id=\"T_5896f_level0_col3\" class=\"col_heading level0 col3\" >speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5896f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5896f_row0_col0\" class=\"data row0 col0\" >A.T@A (float64)</td>\n",
       "      <td id=\"T_5896f_row0_col1\" class=\"data row0 col1\" >0.0155</td>\n",
       "      <td id=\"T_5896f_row0_col2\" class=\"data row0 col2\" >6.5e+10</td>\n",
       "      <td id=\"T_5896f_row0_col3\" class=\"data row0 col3\" >1.0x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5896f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5896f_row1_col0\" class=\"data row1 col0\" >A.T@A (float32)</td>\n",
       "      <td id=\"T_5896f_row1_col1\" class=\"data row1 col1\" >0.0080</td>\n",
       "      <td id=\"T_5896f_row1_col2\" class=\"data row1 col2\" >1.3e+11</td>\n",
       "      <td id=\"T_5896f_row1_col3\" class=\"data row1 col3\" >1.9x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7afbbd8bda80>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Precision experiment: Compare float64 vs float32 matrix multiplication\n",
    "precision_ops = {\n",
    "    'A.T@A (float64)': {\n",
    "        'func': lambda: A_f64.T @ A_f64,\n",
    "        'flops': 2 * n * d**2\n",
    "    },\n",
    "    'A.T@A (float32)': {\n",
    "        'func': lambda: A_f32.T @ A_f32,\n",
    "        'flops': 2 * n * d**2\n",
    "    }\n",
    "}\n",
    "\n",
    "precision_results = []\n",
    "\n",
    "# Time each precision\n",
    "for method_name, operation in precision_ops.items():\n",
    "    start = time.time()\n",
    "    for _ in range(n_repeat):\n",
    "        operation['func']()\n",
    "    end = time.time()\n",
    "    \n",
    "    avg_time = (end - start) / n_repeat\n",
    "    \n",
    "    precision_results.append({\n",
    "        'method': method_name,\n",
    "        'time (s)': avg_time,\n",
    "        'flops/s': operation['flops']/avg_time \n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "precision_df = pd.DataFrame(precision_results)\n",
    "precision_df['speedup'] = precision_df['flops/s'] / precision_df['flops/s'].min()\n",
    "precision_df.style.format({'time (s)': '{:.4f}', 'flops/s':'{:.1e}', 'speedup': '{:.1f}x'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe roughly a 2x speedup by switching from 64-bit double precision to 32-bit single precision.\n",
    "On GPUs, the speedups of using lower-precision can be even more pronounced, as the hardware is often specifically designed to take advantage of lower precision arithmetic.\n",
    "\n",
    "Of course, lower-precision formats have lower precision 🤔, so we must be careful to ensure that the results of our computations are still accurate enough for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-vector queries\n",
    "\n",
    "An increasingly popular way of measuring the cost of numerical algorithms is by counting the number of matrix-vector queries required to complete the algorithm.\n",
    "Here, a matrix-vector query to $\\vec{A}$ is an evaluation of the linear map $\\vec{x} \\mapsto \\vec{A}\\vec{x}$ for some input vector $\\vec{x}$.\n",
    "In some cases we may also have access to transpose queries $\\vec{y} \\mapsto \\vec{A}^\\T\\vec{y}$. \n",
    "\n",
    "Some examples where this access model is natural include:\n",
    "- The arithmetic and runtime costs of Krylov subspace methods like power iteration, conjugate gradient, and Lanczos are often dominated by the cost of matrix-vector products.\n",
    "- A legacy PDE solver may give us access to the action of the solution operator of a PDE, even though we do not have access to the entries of $\\vec{A}$ directly.\n",
    "- Computing matrix-vector products with a Hessian (of e.g. a Neural Network) can be much cheaper than computing the Hessian itself {cite:p}`pearlmutter_94`.\n",
    "\n",
    "There are more theoretical motivations as well. \n",
    "Perhaps the most exciting is the potential for *lower bounds* on the number of matrix-vector queries required to solve a problem.\n",
    "This offers a way to measure of the complexity of various linear algebra tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry evaluations\n",
    "\n",
    "In some settings, evaluating a single entry of a matrix may be the dominant cost. \n",
    "For instance if $\\vec{A}$ is a Kernel matrix, then the $(i,j)$ entry of $\\vec{A}$ takes the form $K(\\vec{x}_i, \\vec{x}_j)$, where $K : \\R^m\\times R^m\\to \\R$ is a function acting on data of dimension $m$, and may be costly to evaluate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}